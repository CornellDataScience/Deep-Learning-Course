{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is for compatibility with Python 2. If you are using Python 3 (recommended), you may ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must import the TensorFlow and numpy packages to be able to use them! We use the prefix \"tf\" to avoid having to type out the full name every time we want to use a TensorFlow command. Likewise, we prefix all numpy commands with \"np\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "tf.set_random_seed(seed)  # Tell TensorFlow to use our seed\n",
    "np.random.seed(seed)      # Tell NumPy to use our seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        if sys.version[0] == '3':\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        elif sys.version[0] == '2':\n",
    "            dict = pickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = unpickle('../cifar-10-data')\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (10000, 3072)\n",
      "y_data shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array(dataset[b'data'])    # The training images\n",
    "y_data = np.array(dataset[b'labels'])  # The labels for the training images\n",
    "print(\"x_data shape:\", x_data.shape)\n",
    "print(\"y_data shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (9000, 3072)\n",
      "x_test shape: (1000, 3072)\n",
      "\n",
      "y_train shape: (9000,)\n",
      "y_test shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "n_samples = x_data.shape[0]\n",
    "indices = np.random.permutation(n_samples)\n",
    "training_idx, test_idx = indices[:int(0.9*n_samples)], indices[int(0.9*n_samples):]\n",
    "\n",
    "x_train, x_test = x_data[training_idx,:], x_data[test_idx,:]\n",
    "y_train, y_test = y_data[training_idx], y_data[test_idx]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print()\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_length = 3072\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(input_tensor, output_features, name='FC', func=tf.nn.relu):\n",
    "    \"\"\"Creates a Fully Connected Layer\n",
    "\n",
    "    Args:\n",
    "        input_tensor:  Tensor of shape `[batch, features]` that this FC layer uses as its input features.\n",
    "        output_features:  The number of features that the layer will output.\n",
    "        name:  The name of the Fully Connected layer. Will use this to define the `tf.variable_scope()`.\n",
    "        func:  The activation function to use. If `None`, uses ReLU.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of (activations, weights, bias). The weights and bias are returned so that a regularizer\n",
    "        can operate directly on them if needed. The activation Tensor represents the output feature \n",
    "        activations. Will have shape `[None, output_features]`.\n",
    "    \"\"\"\n",
    "    input_features = int(input_tensor.shape[1])  # Get the number of features for the input tensor\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('W', initializer=tf.truncated_normal(\n",
    "            shape=[input_features, output_features],\n",
    "            stddev=np.sqrt(2/(input_features*output_features))))  # Set stddev of random distribution for weights\n",
    "        b = tf.get_variable('B', initializer=tf.zeros([output_features]))\n",
    "        \n",
    "        return func(tf.matmul(input_tensor, w) + b, name='Activations'), w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing_error(hidden_features, lmbda):\n",
    "    \n",
    "    print('hidden_features =', hidden_features, 'lambda =', lmbda)\n",
    "    tf.reset_default_graph()  # Clear the graph to avoid errors from reusing variables\n",
    "\n",
    "    with tf.variable_scope('Inputs'):\n",
    "        x = tf.placeholder(tf.float32, [None, input_length], name='x')\n",
    "        y = tf.placeholder(tf.int64, [None,], name='y')  # Last time we one-hot encoded our labels. Now we won't.\n",
    "\n",
    "    with tf.variable_scope('Hidden-Layers'):\n",
    "        #hidden_features = 512\n",
    "        hidden1, w1, _ = fc(x, hidden_features, 'FC1')\n",
    "        hidden2, w2, _ = fc(hidden1, hidden_features, 'FC2')\n",
    "        hidden3, w3, _ = fc(hidden2, hidden_features, 'FC3')\n",
    "\n",
    "    with tf.variable_scope('Softmax'):\n",
    "        w = tf.get_variable('W', initializer=tf.truncated_normal(\n",
    "            shape=[hidden_features, num_classes],\n",
    "            stddev=np.sqrt(1/(hidden_features*num_classes)))) # Set stddev of random distribution for weights\n",
    "        b = tf.get_variable('B', initializer=tf.zeros([num_classes]))\n",
    "\n",
    "        scores = tf.matmul(hidden3, w) + b\n",
    "        # Predicted probability vectors for each sample in the batch, shape = `[None, 10]`\n",
    "        pred = tf.nn.softmax(scores)\n",
    "\n",
    "    with tf.variable_scope('Optimization'):\n",
    "        # Last time we used the regular cross entropy function, but this time we use the \"sparse\" version. \n",
    "        # That's because this version takes care of turning the labels into one hot encodings for us!\n",
    "        loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores))\n",
    "\n",
    "        regularizer = (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3) + tf.nn.l2_loss(w))  # L2 Regularizer\n",
    "        #lmbda = 0.04  # Regularizer coefficient\n",
    "        loss += lmbda*regularizer  # Add regularization penalty to the loss function\n",
    "\n",
    "        correct = tf.equal(tf.argmax(pred, axis=1), y)           # boolean 1-D Tensor of if pred was correct\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  # scalar (0-D) Tensor of the average accuracy\n",
    "\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)  # Op that steps loss towards minimum\n",
    "\n",
    "    init = tf.global_variables_initializer()  # Op that initializes variables\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(init)\n",
    "    \n",
    "    n_epochs = 1   # The number of full passes through the dataset before we quit training\n",
    "    batch_size= 256  # Feed in only 256 images in a single batch instead of all 9,000\n",
    "\n",
    "    training_size = x_train.shape[0]\n",
    "\n",
    "    saver = tf.train.Saver() # Allows us to save a model\n",
    "    # saver.restore(sess, model_name)\n",
    "\n",
    "\n",
    "    for j in range(n_epochs):\n",
    "        perm = np.random.permutation(training_size)  # Every epoch, get a new set of batches\n",
    "        for i in range(0, training_size, batch_size):\n",
    "            idx = perm[i:i+batch_size]  # Select indices for batch\n",
    "            x_batch = x_train[idx]\n",
    "            y_batch = y_train[idx]\n",
    "            sess.run(train_step, feed_dict={x:x_batch, y:y_batch})\n",
    "        if j%50 == 49 or j==0:\n",
    "            l, r, a = sess.run([loss, regularizer, accuracy], feed_dict={x:x_train, y:y_train})\n",
    "            print(\"epoch %6d, loss=%6f, regularizer=%0.4f, accuracy=%.2f%%\" % (j+1, l, round(r, 4), 100*round(a, 4)))\n",
    "            \n",
    "    print('Training Accuracy:', sess.run(accuracy, feed_dict={x:x_train, y:y_train}))\n",
    "    print('Testing Accuracy:', sess.run(accuracy, feed_dict={x:x_test, y:y_test}))\n",
    "    print('')\n",
    "    \n",
    "    return sess.run(accuracy, feed_dict={x:x_test, y:y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysmac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters=dict(hidden_features=('integer',[256, 4096], 512), lmbda=('real', [0, 1], 0.04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_features = 512 lambda = 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9d79f889d648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpysmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMAC_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Lowest function value found: %f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/.local/lib/python3.6/site-packages/pysmac/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, func, max_evaluations, parameter_dict, conditional_clauses, forbidden_clauses, deterministic, num_train_instances, num_test_instances, train_instance_features, num_runs, num_procs, seed, mem_limit_function_mb, t_limit_function_s)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0margument_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscenario_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madditional_options_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mem_limit_smac_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmac_classpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mnum_train_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmem_limit_function_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_limit_function_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmac_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'algo-deterministic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjava_executable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_quality\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpysmac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_smac\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_smac_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margument_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         '''\n\u001b[0;32m--> 260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/anaconda3/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yuji/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = pysmac.SMAC_optimizer()\n",
    "\n",
    "value, parameters = opt.minimize(testing_error, 50, parameters)\n",
    "\n",
    "print(('Lowest function value found: %f'%value))\n",
    "print(('Parameter setting %s'%parameters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
