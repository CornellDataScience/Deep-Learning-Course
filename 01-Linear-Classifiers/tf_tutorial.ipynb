{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Basics Tutorial\n",
    "## What is TensorFlow?\n",
    "TensorFlow is a library for mathematical computation. It has many applications, but is primarily used for neural networks. Although there are wrapper libraries for C++ and Java, we will be using the Python API because it is the most developed.\n",
    "\n",
    "One of the hardest and most tedious tasks when implementing neural networks is getting gradient descent to properly work. The biggest selling point of neural network frameworks like TensorFlow is that it can automatically compute the gradients for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# This is a comment. \n",
    "# Any text after the \"#\" is not run by Python and is there to describe the code.\n",
    "\n",
    "# The next few lines make sure that the code works in both Python 3 and Python 2 \n",
    "# (feel free to ignore if you don't understand).\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "\n",
    "# In order to use TensorFlow, we need to tell Python that we want to use it.\n",
    "import tensorflow as tf     # import tensorflow, any commands will be prefixed with \"tf\"\n",
    "import numpy as np          # import numpy, any commands prefixed with \"np\"\n",
    "\n",
    "# We have some helper code to visualize our code. We won't be explaining how this works ¯\\_(ツ)_/¯\n",
    "from graph_visualize import show_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "A tensor is just a multidimensional array, similar to a ndarray in numpy - any value we use in tensorflow will be a tensor. In the code below, we construct a 2x2 (two by two) Tensor and then multiply each element by 5 and then add 1. Each operation yields a new tensor, which we then reassign the variable `x` to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(2, 2), dtype=int32)\n",
      "Tensor(\"mul:0\", shape=(2, 2), dtype=int32)\n",
      "Tensor(\"add:0\", shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()  # More on this later...\n",
    "\n",
    "# create a new tf.constant called \"x\". It is a 2x2 Tensor. \n",
    "x = tf.constant([[2,4],[1,3]], name='input')    \n",
    "print(x)\n",
    "\n",
    "x = x*5     # multiply each element of \"x\" by 5. Re-assign \"x\" to this new tensor object. \n",
    "print(x)\n",
    "\n",
    "x = x+1     # add 1 to each element of \"x2. Re-assign \"x\" to this new tensor object.\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor object has a value that either we specify directly (like in the 1st example) or is the result of other operations (like the 2nd and 3rd examples). Printing a Tensor displays the following:\n",
    "- name\n",
    "- shape\n",
    "- data type  \n",
    "\n",
    "Much like Java objects or C/C++ pointers, each time we re-assign `x` to a new tensor, the original tensor object still exists on the computer somewhere - we have just made `x` refer to a different tensor. You can see this because the name of the tensor changed. The original Tensors never were modified, instead each time we did an operation on the tensor it created a new Tensor.\n",
    "  \n",
    "More importantly, we cannot get the actual value using the `print()` command. This is because all we have done so far is tell TensorFlow *how* to compute x, but did not tell it to actually *do* any computations. Tensors *represent* values, rather than *being* the actual values themselves. Lets take a look at how TensorFlow internally describes our code so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"height: 700px; width: 100%; border: none\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.20859477294529327&quot;).pbtxt = 'node {\\n  name: &quot;input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;input&quot;\\n  input: &quot;mul/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;mul&quot;\\n  input: &quot;add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.20859477294529327&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph() # A helper function we have to render stuff in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our code did not actually compute anything, it just described the mathematical operations to get to our answer.\n",
    "\n",
    "**Side note: What happens if I rerun the code above without `tf.reset_default_graph()`?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions\n",
    "Ok, the code we wrote so far is basically useless - we don't have a way to actually get any results!\n",
    "\n",
    "To tell TensorFlow to actually compute things, we need to create a session. A session stores the actual state of the code and enables us to fetch results from TensorFlow. A good way to think about a session is as an administrator that manages all the tensors and operations you have defined. It will compute only what needs to be computed, keep track of variables we will need later, and try to optimize the speed of the operations.\n",
    "\n",
    "Lets introduce a few more tensors into the mix and then figure out how to create a session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x in Python: Tensor(\"add:0\", shape=(2, 2), dtype=int32)\n",
      "Value of y in Python: Tensor(\"Const:0\", shape=(), dtype=int32)\n",
      "Value of z in Python: Tensor(\"truediv:0\", shape=(2, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant(2)   # Another constant Tensor. This one is just a scalar (a single number)\n",
    "z = x/y              # Make a new tensor by dividing x (from before) by y. Assign it to z\n",
    "\n",
    "print('Value of x in Python:', x) \n",
    "print('Value of y in Python:', y) \n",
    "print('Value of z in Python:', z) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, here is what our new graph looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"height: 700px; width: 100%; border: none\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.8455090342203553&quot;).pbtxt = 'node {\\n  name: &quot;input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;input&quot;\\n  input: &quot;mul/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;mul&quot;\\n  input: &quot;add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv/Cast_1&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;truediv/Cast&quot;\\n  input: &quot;truediv/Cast_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.8455090342203553&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, *now* we can create a session. To do this, there are two ways to do it: `tf.Session()` and `tf.InteractiveSession()`. For simplicity we suggest using `tf.InteractiveSession()` whenever coding simple programs from the command line or Jupyter, and using `tf.Session()` when using more complicated code. This is because `tf.Session()` is safer but is a little bit more clunky to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession() # To actually compute things, we need to create a session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x in TensorFlow: \n",
      " [[11 21]\n",
      " [ 6 16]] \n",
      " Data type of the result: <class 'numpy.ndarray'> \n",
      "\n",
      "Value of y in TensorFlow: \n",
      " 2 \n",
      " Data type of the result: <class 'numpy.int32'> \n",
      "\n",
      "Value of z in TensorFlow: \n",
      " [[ 5.5 10.5]\n",
      " [ 3.   8. ]] \n",
      " Data type of the result: <class 'numpy.ndarray'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The eval() function asks the default session to compute the value of the Tensor and return the result.\n",
    "x_eval = x.eval() \n",
    "y_eval = y.eval()\n",
    "z_eval = z.eval()\n",
    "\n",
    "print('Value of x in TensorFlow: \\n', x_eval, '\\n Data type of the result:', type(x_eval), '\\n')\n",
    "print('Value of y in TensorFlow: \\n', y_eval, '\\n Data type of the result:', type(y_eval), '\\n')\n",
    "print('Value of z in TensorFlow: \\n', z_eval, '\\n Data type of the result:', type(z_eval), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, notice how `print(x)` can't give us the value, but `print(x.eval())` does! `x.eval()` returns a numpy ndarray because now TensorFlow needs a way to show the actual value as a multidimensional array, and numpy is the best way to do that.\n",
    "\n",
    "Although `x.eval()` is a convenient way to evaluate `x`, it is more common instead to use the `Session.run()` method. `Session.run()` takes a list of Tensors as its first argument, and then evaluates each and returns them. This list of Tensors is called the \"fetches\". It will return another list that corresponds to the fetched result of each of the tensors in `fetches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x in TensorFlow: \n",
      " [[11 21]\n",
      " [ 6 16]] \n",
      "\n",
      "Value of y in TensorFlow: \n",
      " 2 \n",
      "\n",
      "Value of z in TensorFlow: \n",
      " [[ 5.5 10.5]\n",
      " [ 3.   8. ]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Python, you can unpack a list like this: `a,b,c = [x1,x2,x3]`\n",
    "# Since sess.run is asked to fetch the values of 3 Tensors, it returns a list of 3 ndarrays.\n",
    "# They are then assigned to each of `x_eval`, `y_eval`, and `z_eval`\n",
    "x_eval, y_eval, z_eval = sess.run([x, y, z])\n",
    "\n",
    "print('Value of x in TensorFlow: \\n', x_eval, '\\n')\n",
    "print('Value of y in TensorFlow: \\n', y_eval, '\\n')\n",
    "print('Value of z in TensorFlow: \\n', z_eval, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs\n",
    "Ok, we now know how to get results from TensorFlow. Lets backtrack for a second and go back to those interactive diagrams we rendered. To refresh your memory, here is what we have so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"height: 700px; width: 100%; border: none\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.8610886731371327&quot;).pbtxt = 'node {\\n  name: &quot;input&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\002\\\\000\\\\000\\\\000\\\\004\\\\000\\\\000\\\\000\\\\001\\\\000\\\\000\\\\000\\\\003\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 5\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;input&quot;\\n  input: &quot;mul/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;add&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;mul&quot;\\n  input: &quot;add/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 2\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;add&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv/Cast_1&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;Const&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;truediv/Cast&quot;\\n  input: &quot;truediv/Cast_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_DOUBLE\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.8610886731371327&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how there are circles and lines? Each circle represents a store of some value, and is typically a Tensor or an Operation. Each line represents dataflow. Hence, the diagram is actually a graph. TensorFlow stores this graph in a `tf.Graph` object. We get one by default whenever we write code, which is why we never had to construct a `tf.Graph` object (in larger more complex programs it is often good practice to explicitly make one).\n",
    "\n",
    "A Tensorflow Graph is how TensorFlow internally represents and remembers all the operations and Tensors that you define. TensorFlow uses the graph to know exactly which Tensors to compute based on the fetches you provide in the `session.run()` method. TensorFlow automatically uses a default graph which is why we never had to call any functions to make one, although we can manually specify one if we want to.\n",
    "\n",
    "Below is the graph for a simple Logistic Regression model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='http://ischlag.github.io/images/graph_mess.png', alt='Image from Imanol Schlag blog'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( ͡°_ʖ ͡°) Wow thats pretty complicated actually. It's pretty hard to tell what is going on here, so is there a way to organize these ops and tensors into a logically sensible hierarchy?\n",
    "\n",
    "Luckily TensorFlow has a way for us to group our tensors and operations together using `tf.variable_scope()`. We will get into how to actually use it at another time, but here is the same graph after grouping some of these operations together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src ='http://ischlag.github.io/images/graph_example.png', alt='Image from Imanol Schlag blog'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "( ͡~ ͜ʖ ͡°) Much better.\n",
    "\n",
    "Each node in the graph takes in Tensors and outputs Tensors. The lines (directed edges) in the graph represent the flow of data through the graph, which means they also tell TensorFlow which operations depend on which. This is how TensorFlow knows that to compute the loss value, it first needs to compute the softmax function, and the matrix multiplication, etc. The graph can also be cleared at any time using `tf.reset_default_graph()`.\n",
    "\n",
    "Don't worry too much about the details of this specific graph!\n",
    "\n",
    "## TensorFlow Objects\n",
    "So far we have talked about Tensors and Operations. We have also used a `tf.constant` function. Lets get a little more specific:\n",
    "\n",
    "### Constants\n",
    "A `tf.constant` is a tensor that does not change: it remains the same throughout the duration of a Session. It can be used to store values and is hard coded into the graph. \n",
    "\n",
    "### Placeholders\n",
    "In our first example, we used a `tf.constant` as the source of the data we wished to operate on - the problem with this is that data can never change. What we really want is a way for us to be able to have some Tensor that acts as a placeholder for our data, so that we can then feed in whatever data we have (e.g. new training/test data!).\n",
    "\n",
    "This is what the `tf.placeholder` is for. A placeholder is a Tensor that can be used to feed data into, and cannot be evaluated directly until it receives data. Once we have it, we can feed values into it by using the `feed_dict` argument of the `session.run()` method. We pass a Python dict into the `feed_dict` argument, where the dict tells us which placeholders are receiving which data. If we try to evaluate a Tensor that depends on a tf.placeholder without properly feeding in data using the `feed_dict` argument, TensorFlow will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"x:0\", shape=(?, 2), dtype=float32)\n",
      "Tensor(\"pow:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sess.close()                 # close the old session to get rid of all the resources associated with it\n",
    "tf.reset_default_graph()     # get rid of the previous graph\n",
    "\n",
    "# shape=[None,2] means that the data can have any sized first dimension, but must have second dimension size of 2.\n",
    "x = tf.placeholder(tf.float32, shape=[None, 2], name='x')\n",
    "print(x)\n",
    "\n",
    "x2 = x**2                    # square each element of x\n",
    "print(x2)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we had to specify a shape for `x`. This is because TensorFlow wants to know beforehand the shape of the Tensor so that it can properly define the graph. Because we set the shape as `[None, 2]`, `x` can be fed data of shape `[1,2]` or `[1337,2]`, but not `[2,3]` or `[10,4]` or `[12, 2, 2]` for example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error! We did not properly feed in a value for the placeholder\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "try:\n",
    "    sess.run(x2)             # will throw an error\n",
    "    print(\"Success? LOL\")        # will never happen since error occurs first\n",
    "\n",
    "except Exception:     # if an error occurs, the following code should execute\n",
    "    print('Error! We did not properly feed in a value for the placeholder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an error because when we try to evaluate `x2`, TensorFlow looks at the graph and sees that it depends on `x`. But we never fed any data into `x`!   \n",
    "Now let's do it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:\n",
      " [[ 4. 16.]\n",
      " [ 1.  9.]]\n",
      "\n",
      " Result 2:\n",
      " [[ 25.  36.]\n",
      " [ 49.  64.]\n",
      " [ 81. 100.]]\n"
     ]
    }
   ],
   "source": [
    "input_1 = np.array([[2,4],\n",
    "                    [1,3]])  # 2x2 ndarray (numpy multidimensional array) as input\n",
    "\n",
    "input_2 = np.array([[5,6],\n",
    "                    [7,8],\n",
    "                    [9,10]]) # 3x2 ndarray\n",
    "\n",
    "# In python, a dict is a structure that maps keys to values\n",
    "# e.g. {key1: value1, key2: value2, ...}\n",
    "# {x: input_1} is a dict that says Tensor `x` will be fed data from the array `input_1`\n",
    "\n",
    "print('Result 1:\\n', sess.run(x2, feed_dict={x: input_1}))\n",
    "print('\\n Result 2:\\n', sess.run(x2, feed_dict={x: input_2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "So we have `tf.constant` which always keeps the same value, and `tf.placeholder` which we can manually set the value of for each `session.run()` call. However, we have nothing that can hold a state between `session.run()` calls that can be updated and read from by TensorFlow. To fix this, we will use a `tf.Variable`. \n",
    "\n",
    "You can use a `tf.Variable` anywhere you would use a Tensor, although you can also update the `tf.Variable` using TensorFlow operations. Unlike Tensors, a `tf.Variable` holds state between multiple `session.run()` calls.\n",
    "\n",
    "We primarily use `tf.Variable` to hold our parameters (weights, bias, etc) because TensorFlow can update them over the course of several `session.run()` calls. To create one, you must give it an \"initializer\", which can just be some other Tensor. This is what the `tf.Variable` will use as its initial value, although it can be updated later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x: <tf.Variable 'X:0' shape=(10, 2) dtype=float32_ref>\n",
      "Type of x: <class 'tensorflow.python.ops.variables.Variable'>\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()    # clear graph\n",
    "sess.close()                # close the previous session\n",
    "\n",
    "# Create the `tf.Variable from a random Tensor of shape [10,2]\n",
    "x = tf.Variable(tf.random_normal([10,2]), name=\"X\")  #\n",
    "print(\"Value of x:\", x)\n",
    "print(\"Type of x:\", type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, a `tf.Variable` has to be initialized before using it in a session. The following code will throw an error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error!\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    print(sess.run(x))\n",
    "    print('Success?')  # Won't ever reach this because of the error\n",
    "except Exception:\n",
    "    print('Error!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this reason we must call `tf.global_variables_initializer()` before trying to evaluate the `tf.Variable`. This runs the operation or tensor that was specified as the initializer for the `tf.Variable`, and uses it to assign the `tf.Variable` its initial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6494354   1.0536859 ]\n",
      " [ 1.7155834  -0.42719492]\n",
      " [-1.0376663  -1.5082024 ]\n",
      " [-1.4290298  -1.4264525 ]\n",
      " [-0.24083471 -0.5607169 ]\n",
      " [-0.5927587  -0.01444115]\n",
      " [-0.13221323 -0.4364146 ]\n",
      " [ 0.32954663 -0.08297483]\n",
      " [-0.21678616 -0.8222877 ]\n",
      " [-0.78208065 -0.23409903]]\n"
     ]
    }
   ],
   "source": [
    "# This is just a TensorFlow operation. Nothing is run just yet.\n",
    "init = tf.global_variables_initializer()  \n",
    "\n",
    "sess.run(init)  # Now we actually run the initialization op\n",
    "\n",
    "print(sess.run(x))  # Evaluate the `tf.Variable`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might still not be clear how this is any different than what we have been doing so far, however the true power of `tf.Variable` will become evident in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization and Calculus\n",
    "One strength of Tensorflow is that the programmer doesn't have to know how to calculate the gradient of the function being optimized. Instead, the library comes with several optimizers commonly used in deep learning, including Gradient Descent, Momentum, and ADAM. Below we use an optimizer to minimize the function $y=x^2$ given a random initial $x$. Obviously the correct value is for $x$ to just be zero, so lets see if TensorFlow figures that out. Our first step will be to define the function we want to optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()            # clear the graph\n",
    "sess.close()                        # close the previous session\n",
    "sess = tf.InteractiveSession()      # start a new one\n",
    "\n",
    "x = tf.Variable(5.0)                # let x be a variable with initial value x=5.0\n",
    "y = x**2                            # let y = x^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"height: 700px; width: 100%; border: none\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.125309428397951&quot;).pbtxt = 'node {\\n  name: &quot;Variable/initial_value&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 5.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;Variable&quot;\\n  input: &quot;Variable/initial_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;Variable/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;Variable&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@Variable&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;pow/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 2.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;pow&quot;\\n  op: &quot;Pow&quot;\\n  input: &quot;Variable/read&quot;\\n  input: &quot;pow/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.125309428397951&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to tell TensorFlow to minimize the value of $y$. Once we do this, TensorFlow will look at every `tf.Variable` and see if $y$ depends at all on that variable. In this case, $y$ depends on $x$, so TensorFlow will modify $x$ in order to minimize $y$. The actual mathematical process by which this ocurrs is called Gradient Descent, which we covered in lecture (if you didn't, see [this](https://youtu.be/5u0jaA3qAGk) for a super brief explanation). In order to achieve this, TensorFlow automatically takes the gradient of the value you want to minimize with respect to all relevant `tf.Variable`s in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : x = 5.000 y = 25.000\n",
      "Step 5 : x = 1.638 y = 2.684\n",
      "Step 10 : x = 0.537 y = 0.288\n",
      "Step 15 : x = 0.176 y = 0.031\n",
      "Step 20 : x = 0.058 y = 0.003\n",
      "Step 25 : x = 0.019 y = 0.000\n",
      "Step 30 : x = 0.006 y = 0.000\n",
      "Step 35 : x = 0.002 y = 0.000\n"
     ]
    }
   ],
   "source": [
    "# gradient descent to minimize y, with learning rate=0.1\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(y)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())     # initialize all variables\n",
    "\n",
    "for i in range(40):                             # run 40 training steps of gradient descent\n",
    "    x_val, y_val = sess.run([x, y])\n",
    "    if i%5 == 0:\n",
    "        print ('Step', '{:1}'.format(i), ': x =', '{0:.3f}'.format(x_val), 'y =', '{0:.3f}'.format(y_val))\n",
    "    sess.run(train_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value of the variable `x` changes with training, and converges to (nearly) the global minimum at $x=0$ as expected; it is not exact because gradient descent is an approximation.  The Session is still responsible for running the optimization operation, and a list of operations/tensors can be passed to the `session.run()` function to extract multiple values from the model. We don't need a feed_dict because there are no placeholders defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End!\n",
    "After this super quick primer to TensorFlow, you should be able to start writing your own programs. For some additional tutorials, check out [this](https://youtu.be/wuo4JdG3SvU) fantastic series, with code [here](https://github.com/Hvass-Labs/TensorFlow-Tutorials). You can also find detailed info about any of TensorFlow's API [here](https://www.tensorflow.org/api_docs/python)."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
