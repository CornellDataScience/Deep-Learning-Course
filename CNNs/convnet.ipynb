{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network: VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create several helper functions to create the layers of our convolutional neural network:   \n",
    "    \n",
    "`conv` takes as its arguments the input volume, the number of filters in the convolutional layer, and a name we assign it. Since we are using pre-trained weights we use the `tf.constant_initializer()`, but when starting from scratch you will want to use `tf.truncated_normal()` or the `xavier_initializer()`. We return the (normalized) output volume, which willl be the input volume to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(inputs, n_filters, name):\n",
    "    weights, bias, beta, gamma, mov_mean, mov_var = load_weights(name)\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', initializer=weights)\n",
    "        b = tf.get_variable('b', initializer=bias)\n",
    "        convolved = tf.nn.conv2d(input=inputs,\n",
    "                                 filter=W,\n",
    "                                 strides=[1,1,1,1],\n",
    "                                 padding='SAME')\n",
    "        activations = tf.nn.relu(convolved + b)\n",
    "        normed = tf.nn.batch_normalization(activations, mov_mean, mov_var, beta, gamma, 1e-7)\n",
    "        return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pool` creates a pooling layer - you will notice that a pooling layer does not have any learned parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(inputs, name):\n",
    "    with tf.variable_scope(name):\n",
    "        return tf.nn.max_pool(inputs, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fc` creates a fully connected layer as we did before - we compute the weighted sum of pixels, add a bias term, pass the scores through an activation function (we use ReLU), and then normalize the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(inputs, name):\n",
    "    weights, bias, beta, gamma, mov_mean, mov_var = load_weights(name)\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', initializer=weights)\n",
    "        b = tf.get_variable('b', initializer=bias)\n",
    "        activations = tf.nn.relu(tf.matmul(inputs, W) + b)\n",
    "        normed = tf.nn.batch_normalization(activations, mov_mean, mov_var, beta, gamma, 1e-7)\n",
    "        return normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`softmax` creates a softmax layer, which allows us to convert scores into easily interpretable probabilities. Note that we will only ever use the softmax layer at the very end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(inputs, name):\n",
    "    weights, bias = load_weights(name)\n",
    "    with tf.variable_scope(name):\n",
    "        W = tf.get_variable('W', initializer=weights)\n",
    "        b = tf.get_variable('b', initializer=bias)\n",
    "        return tf.nn.softmax(tf.matmul(inputs, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 32\n",
    "IMAGE_HEIGHT = 32\n",
    "COLOR_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pre-trained weights have been saved as a .npz file, which can be loaded using the `np.load` function - this returns a dict similar to what we get with the CIFAR-10 data. Once we have loaded the weights, we use a helper function to return the appropriate weights given a layer name. For each layer, we have a `kernel` ($W$), `bias` ($b$), and normalization parameters $\\beta, \\gamma$, the moving average and the moving variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WEIGHTS = 'cifar10vgg_numpy.npz'\n",
    "weights = np.load(WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_weights(layer_name):\n",
    "    layer_type = layer_name.split('_')[0]\n",
    "    layer_num = int(layer_name.split('_')[1])\n",
    "    W = weights[\"b'\" + str(layer_name) + \"/kernel:0'\"]\n",
    "    b = weights[\"b'\" + str(layer_name) + \"/bias:0'\"]\n",
    "    if layer_type == 'conv2d':\n",
    "        beta = weights[\"b'batch_normalization_\" + str(layer_num) + \"/beta:0'\"]\n",
    "        gamma = weights[\"b'batch_normalization_\" + str(layer_num) + \"/gamma:0'\"]\n",
    "        mov_mean = weights[\"b'batch_normalization_\" + str(layer_num) + \"/moving_mean:0'\"]\n",
    "        mov_var = weights[\"b'batch_normalization_\" + str(layer_num) + \"/moving_variance:0'\"]\n",
    "        return W, b, beta, gamma, mov_mean, mov_var\n",
    "    elif layer_type == 'dense':\n",
    "        if layer_num == 1:\n",
    "            beta = weights[\"b'batch_normalization_14/beta:0'\"]\n",
    "            gamma = weights[\"b'batch_normalization_14/gamma:0'\"]\n",
    "            mov_mean = weights[\"b'batch_normalization_14/moving_mean:0'\"]\n",
    "            mov_var = weights[\"b'batch_normalization_14/moving_variance:0'\"]\n",
    "            return W, b, beta, gamma, mov_mean, mov_var\n",
    "        else:\n",
    "            return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the architecture of our convolutional network, as in the diagram below (with a slight modification of the fully connected layers):     \n",
    "![alt text](vgg16.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = tf.placeholder(tf.float32, shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, COLOR_CHANNELS], name='img')\n",
    "\n",
    "conv1_1 = conv(img, 64, 'conv2d_1')\n",
    "conv1_2 = conv(conv1_1, 64, 'conv2d_2')\n",
    "pool1 = pool(conv1_2, 'pool1')\n",
    "\n",
    "conv2_1 = conv(pool1, 128, 'conv2d_3')\n",
    "conv2_2 = conv(conv2_1, 128, 'conv2d_4')\n",
    "pool2 = pool(conv2_2, 'pool2')\n",
    "\n",
    "conv3_1 = conv(pool2, 256, 'conv2d_5')\n",
    "conv3_2 = conv(conv3_1, 256, 'conv2d_6')\n",
    "conv3_3 = conv(conv3_2, 256, 'conv2d_7')\n",
    "pool3 = pool(conv3_3, 'pool3')\n",
    "\n",
    "conv4_1 = conv(pool3, 512, 'conv2d_8')\n",
    "conv4_2 = conv(conv4_1, 512, 'conv2d_9')\n",
    "conv4_3 = conv(conv4_2, 512, 'conv2d_10')\n",
    "pool4 = pool(conv4_3, 'pool4')\n",
    "\n",
    "conv5_1 = conv(pool4, 512, 'conv2d_11')\n",
    "conv5_2 = conv(conv5_1, 512, 'conv2d_12')\n",
    "conv5_3 = conv(conv5_2, 512, 'conv2d_13')\n",
    "pool5 = pool(conv5_3, 'pool5')\n",
    "\n",
    "flattened = tf.reshape(pool5, [-1, 512])\n",
    "fc6 = fc(flattened, 'dense_1')\n",
    "pred = softmax(fc6, 'dense_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we use the a helper function to allow us to load CIFAR-10 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        if sys.version[0] == '3':\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        elif sys.version[0] == '2':\n",
    "            dict = pickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network we are using was trained on standardized data, meaning that all the inputs were scaled so that the mean of the training data was 0 and the standard deviation was 1. This is a technique that allows us to train the model faster, because if we work with strictly positive input values from 0-255, we have a strong bias towards highly positive activations if we randomly initialize our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN = 120.707\n",
    "STD = 64.15\n",
    "\n",
    "def normalize(X_test):\n",
    "    return (X_test - MEAN)/(STD + 1e-7)\n",
    "\n",
    "def denormalize(X_out):\n",
    "    return (X_out)*(STD + 1e-7) + MEAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the performance of our convolutional network on the CIFAR-10 data we have been using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar10 = unpickle('../cifar-10-data')\n",
    "permutation = np.random.permutation(10000)\n",
    "data = np.array(cifar10[b'data'])[permutation]\n",
    "labels = np.array(cifar10[b'labels'])[permutation]\n",
    "\n",
    "n_images = 300\n",
    "x_test = normalize(data.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\"))[:n_images]\n",
    "y_test = labels[:n_images]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a placeholder to feed in the labels, and define `correct` and `accuracy` so we can analyze the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int64, [None,], name='y')\n",
    "correct = tf.equal(tf.argmax(pred, axis=1), y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 data: 100.0%\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "acc = sess.run(accuracy, feed_dict={img:x_test, y:y_test})\n",
    "\n",
    "print('Accuracy on CIFAR-10 data: ' +  str(100*round(acc, 4)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our model is definitely very powerful, this measure is deceiving as the network was trained using this exact data! We can't tell if our model simply \"memorized\" all the images in the dataset, or if it generalizes to other images as well. Let's now test the model using data that it was not trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cifar10 = unpickle('test_batch')\n",
    "permutation = np.random.permutation(10000)\n",
    "data = np.array(cifar10[b'data'])[permutation]\n",
    "labels = np.array(cifar10[b'labels'])[permutation]\n",
    "\n",
    "n_images = 300\n",
    "x_test = normalize(data.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\"))[:n_images]\n",
    "y_test = labels[:n_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 test data: 94.3300008774%\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "acc = sess.run(accuracy, feed_dict={img:x_test, y:y_test})\n",
    "\n",
    "print('Accuracy on CIFAR-10 test data: ' +  str(100*round(acc, 4)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy on the testing set is still very high! Convolutional neural networks are the state-of-the-art architecture for image classification. In case you still don't believe these results, we can test on arbitrary images below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages below will be used to load images and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helper funtion that will read in a local image file from your computer and create a tensor with dimensions [1, height, width, 3] that can be fed into our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    image = scipy.misc.imread(path)\n",
    "    image = np.reshape(image, ((1,) + image.shape)) \n",
    "    image = normalize(image)                     \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your own images by changing the file path `IMG` below. (Note that your images must be 32x32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "IMG = 'dog-small.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: dog\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZ8udcBPEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3d87272048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_image = load_image(IMG)\n",
    "prediction = np.argmax(sess.run(pred, feed_dict={img:test_image}))\n",
    "\n",
    "plt.imshow(255 - denormalize(test_image[0]));\n",
    "print('Prediction:', label_names[prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further train the network using your own data, you can create a labeled dataset of your own and use the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scores(inputs, name):\n",
    "    W, b = load_weights(name)\n",
    "    W = tf.constant(W)\n",
    "    b = tf.constant(np.reshape(b, (b.size)))\n",
    "    return tf.matmul(inputs, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = scores(fc6, 'dense_2')\n",
    "with tf.variable_scope('Optimization'):\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=s))    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c41f8546b001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size= 256\n",
    "\n",
    "training_size = x_train.shape[0]\n",
    "\n",
    "for j in range(n_epochs):\n",
    "    perm = np.random.permutation(training_size) \n",
    "    for i in range(0, training_size, batch_size):\n",
    "        idx = perm[i:i+batch_size]  \n",
    "        x_batch = x_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        sess.run(train_step, feed_dict={x:x_batch, y:y_batch})\n",
    "    if j%50 == 49 or j==0:\n",
    "        l, r, a = sess.run([loss, regularizer, accuracy], feed_dict={x:x_train, y:y_train})\n",
    "        print(\"epoch %6d, loss=%6f, regularizer=%0.4f, accuracy=%.2f%%\" % (j+1, l, round(r, 4), 100*round(a, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
