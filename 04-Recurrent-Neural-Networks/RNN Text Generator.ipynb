{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks in TensorFlow\n",
    "\n",
    "This week we will be using Recurrent Neural Networks, or RNNs, to generate text. The model that we have built is a rather simple and naive approach to generating text. There are better and more effective methods for text generation but we will not be exploring them today.\n",
    "\n",
    "Lets jump into the code!\n",
    "\n",
    "## Loading the Data\n",
    "Before we do anything, we must load the data. The data we are using is from the book \"Alice in Wonderland\". We have already prepared some numpy files that contain the data to make things easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 31337\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embed_output(data, vocab):\n",
    "    \"\"\"Takes a list of words and outputs a list of the indices of the words in the vocab list\"\"\"\n",
    "    result = np.empty(len(data))\n",
    "    for idx, word in enumerate(data):\n",
    "        result[idx] = int(np.where(vocab == data[idx])[0])  # Get index of word in vocab array\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `embed_output` function to convert them into numbers based on the vocabulary, which is just a list of words. The `embed_output` function will just convert the list of words into a list of numbers. Obviously this is important because Neural Networks understand numbers and not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alice', 's', 'adventures', 'in', 'wonderland', 'by', 'lewis',\n",
       "       'carroll', 'chapter', 'i', '.', 'down', 'the', 'rabbit', 'hole',\n",
       "       'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of',\n",
       "       'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', 'and', 'of',\n",
       "       'having', 'nothing', 'to', 'do', 'once', 'or', 'twice', 'she',\n",
       "       'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was',\n",
       "       'reading', 'but', 'it', 'had', 'no', 'pictures', 'or',\n",
       "       'conversations', 'in', 'it', 'and', 'what', 'is', 'the', 'use',\n",
       "       'of', 'a', 'book', 'thought', 'alice', 'without', 'pictures', 'or',\n",
       "       'conversation', 'so', 'she', 'was', 'considering', 'in', 'her',\n",
       "       'own', 'mind', 'as', 'well', 'as', 'she', 'could', 'for', 'the',\n",
       "       'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and',\n",
       "       'stupid', 'whether', 'the', 'pleasure', 'of'],\n",
       "      dtype='<U14')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"saved/rnn.ckpt\"\n",
    "alice_file = \"alice_with_periods.npz\"\n",
    "\n",
    "# Load alice in wonderland into an array of numbers called `alice_embed`\n",
    "alice_load = np.load(alice_file)\n",
    "alice_embed = embed_output(alice_load['words'], alice_load['vocab'])  # Alice as sequence of ints, shape [num_words]\n",
    "embedding_size = len(alice_load['vocab'])\n",
    "alice_load['words'][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to break the list of words up into a list of sentences. Each sentence will be `sequence_length` units long. We then split the dataset into training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice_train.shape: (842, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1899.,  1379.,   823., ...,   581.,   402.,  2556.],\n",
       "       [  268.,  1336.,  1584., ...,  2280.,   265.,   436.],\n",
       "       [ 1664.,  1819.,   740., ...,  1373.,   699.,   740.],\n",
       "       ..., \n",
       "       [ 1104.,   593.,  2280., ...,  2556.,  1122.,   788.],\n",
       "       [  823.,  1916.,  1104., ...,  2264.,   270.,  2387.],\n",
       "       [  175.,   823.,  1822., ...,   788.,   325.,  1599.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split\n",
    "sequence_length = 25  # The length of the generated sequences\n",
    "truncated_length = (len(alice_embed)//sequence_length) * sequence_length  # Need to make everything divisible\n",
    "alice_embed = alice_embed[:truncated_length]  # truncate the list of words to be divisible by `sequence_length`\n",
    "alice_split = np.reshape(alice_embed, (-1, sequence_length))  # break the text into fixed length sentences\n",
    "num_sequences = alice_split.shape[0]\n",
    "\n",
    "\n",
    "indices = np.random.permutation(num_sequences)  # Randomly re-order the list of sentences\n",
    "pct_train = 0.75\n",
    "# Split dataset into training and testing\n",
    "training_idx, test_idx = indices[:int(pct_train*num_sequences)], indices[int(pct_train*num_sequences):]\n",
    "alice_train, alice_test = alice_split[training_idx, :], alice_split[test_idx, :]\n",
    "print(\"alice_train.shape:\", alice_train.shape)\n",
    "alice_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN\n",
    "Ok, we have the data but now we need to design the network architecture in TensorFlow. Lets start with the code needed to design the training architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # Just in case we want to re-run the jupyter cell, this avoids getting errors\n",
    "tf.set_random_seed(seed)  # For debugging purposes, make everyone use the same seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our input and call it `x`. Then we generate one-hot encodings of `x` and put them in `x_hot`. One-hot encodings are important because just like in image recognition where you need to one-hot encode the labels so that there is an index associated with each class, here we need to one-hot encode the inputs so that there is an index associated with each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input sequence placeholder [batch_size, sequence_length]\n",
    "x = tf.placeholder(shape=[None, sequence_length], dtype=tf.int32)\n",
    "# one-hot encode x shaped `[batch_size, sequence_length, embedding_size]`\n",
    "x_hot = tf.one_hot(x, depth=embedding_size)\n",
    "batch_size = tf.shape(x)[0]  # This is a scalar tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now is the tricky part. Recall that recurrent neural networks have cyclic graphs, where the outputs of the RNN get fed back into the RNN in a feedback cycle. In order to perform backpropogation, we need to \"unroll\" the RNN so that there are no loops in the graph. We achieve this by operating on sequences of a fixed length, and then just pass each element of that sequence through the RNN cell. This turns it into a feed-forward neural network instead of a recurrent neural network. This can be seen in the for loop in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('Unrolled') as scope:\n",
    "    num_units = 256  # number of RNN units in a the RNN cell. This is the size of the state vectors used in the cell.\n",
    "    # Make three different LSTM cells\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "    cell3 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "\n",
    "    # The state of the RNN is initialized to (usually) zero at the start of every sequence.\n",
    "    state = cell1.zero_state(batch_size=batch_size, dtype=tf.float32)  # The intial state of the RNN\n",
    "\n",
    "    # Unroll the graph by passing the data into the RNN starting from the 1st element of the sequence until the last\n",
    "    outputs = []  # python list of tensors so we can keep track of each timestep\n",
    "    # We subtract one because when we generate the last word, we dont feed it back in.\n",
    "    for i in range(sequence_length-1):\n",
    "        # NOTE: each time we call the cell, it calls tf.get_variable internally. If scope.reuse is not True, it will\n",
    "        # create new variables at each word in the sentence. We therefore want to stop creating those variables once\n",
    "        # we have called each cell once.\n",
    "        if i > 0: scope.reuse_variables()  # Reuse the parameters created in the 1st RNN cell\n",
    "\n",
    "        # Just like in a feed forward network where more layers = more powerful network, we have several\n",
    "        # LSTM cells. This enables the network to be able to learn highly non-linear functions.\n",
    "        output, state = cell1(x_hot[:, i, :], state, scope='Cell1')  # Use the word as the input (1st layer)\n",
    "        output, state = cell2(output, state, scope='Cell2')  # feed output and state from cell1 to cell2 (2nd layer)\n",
    "        output, state = cell3(output, state, scope='Cell3')  # feed output and state from cell2 to cell3 (3rd layer)\n",
    "        outputs.append(output)  # Append the outputs to the python list of tensors\n",
    "\n",
    "    # Turn the python list back into a tensor. \n",
    "    # This \"stacks\" them into a tensor of shape `[batch_size, sequence_length, cell3.output_size]`\n",
    "    outputs = tf.stack(outputs, axis=1, name='Outputs')  # Axis=1 makes the sequence dimension the 2nd dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finished the recurrent parts of the network, we need to generate probabilites. Right now, the network is generating scores vectors that are of size `cell3.output_size`, but we want score vectors that are of size `embedding_size`. To achieve these requirements, we will use a softmax layer and apply it to the last dimension of `outputs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we want to get probabilities out of the lstm for each word, so we use a softmax layer.\n",
    "with tf.variable_scope('Softmax'):\n",
    "    # Note that we need to go from vectors of size `cell3.output_size` to `embedding_size`\n",
    "    w = tf.get_variable(\n",
    "        name='Weight',\n",
    "        initializer=tf.truncated_normal([cell3.output_size, embedding_size], stddev=0.01))\n",
    "    b = tf.get_variable(name='Bias', initializer=tf.zeros(embedding_size))\n",
    "    \n",
    "    # We flattten from shape `[batch_size, sequence_length, cell3.output_size]` \n",
    "    # to `[batch_size*sequence_length, embedding_size]` because broadcasting doesn't work properly for `tf.matmul`\n",
    "    flattened = tf.reshape(outputs, (-1, cell3.output_size))\n",
    "    # Do multiplication and then reshape into `[batch_size, sequence_length, embedding_size]`\n",
    "    matmul = tf.reshape(tf.matmul(flattened, w), shape=(-1, sequence_length-1, embedding_size))\n",
    "    # Add the bias term to get the tensor of class scores\n",
    "    scores = tf.add(matmul, b, name='Scores')\n",
    "    # Run the scores through the softmax function to generate probabilities\n",
    "    softmax = tf.nn.softmax(scores, name='Softmax')\n",
    "\n",
    "# Shift over the inputs to create the labels. At timestep 1, input is `x[:,1,:]` and target output is `x[:,2,:]`\n",
    "# Note that the shape of labels is `[batch_size, sequence_length]`. \n",
    "# There is no dimension for the embedding because the labels were not one-hot encoded.\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=scores, labels=x[:, 1:]))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to actually feed data into the model and get it to train. This took a long time, so we have jumpstarted the process by providing a pre-trained model. The following code loads that model and trains it further for a couple of epochs (passes through the full training set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from saved/rnn.ckpt\n",
      "Restored Model!\n",
      "epoch      1, loss=1.928284\n",
      "epoch      2, loss=1.883919\n",
      "epoch      3, loss=1.707575\n",
      "epoch      4, loss=1.638360\n",
      "epoch      5, loss=1.570829\n",
      "epoch      6, loss=1.477021\n",
      "epoch      7, loss=1.400826\n",
      "epoch      8, loss=1.404719\n",
      "epoch      9, loss=1.298011\n",
      "epoch     10, loss=1.283630\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()  # we will use this to load the variables from the ckpt files\n",
    "\n",
    "training_size = alice_train.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "try:\n",
    "    # Try to load the saved model\n",
    "    saver.restore(sess, save_path)\n",
    "    print(\"Restored Model!\")\n",
    "except Exception:\n",
    "    # The saved model did not exist, so randomly initialize the model parameters to start training from scratch\n",
    "    print(\"Initializing Model!\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_epochs = 10  # number of training epochs. An epoch is a full pass through the whole training set\n",
    "num_batches = training_size // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    perm = np.random.permutation(training_size)  # Every epoch, get a new set of batches\n",
    "    avg_loss = 0\n",
    "    for i in range(0, training_size, batch_size):\n",
    "        idx = perm[i:i + batch_size]  # Select indices for batch\n",
    "        x_batch = alice_train[idx]\n",
    "        _, batch_loss = sess.run([train_step, loss], feed_dict={x: x_batch})\n",
    "        avg_loss += batch_loss\n",
    "    print(\"epoch %6d, loss=%6f\" % (epoch + 1, avg_loss/num_batches))\n",
    "\n",
    "# Save model here if we wanted to\n",
    "# print(\"Saving model to %s\" % save_path)\n",
    "# saver.save(sess, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text\n",
    "Now that we have trained the model, we need to generate text. Unfortunately, we can't directly use the same tensors as we did when we trained the model because it tries to fit the original data rather than create new data. We must recreate the architecture again, but with slight modifications, to be able to actually generate new data.\n",
    "\n",
    "We will tell the architecture how many sentences we want to generate with `num_to_generate`. This will become the size of our batch dimension. Then, we select a random word from the vocabulary and start each sentence with it. We also load the weights from the softmax layer here too so that we can use them when we unroll the RNN. Note that because the session that we used from training still is active, when we call `tf.get_variable` it will use the same variables made in the training code. Note that `reuse=True` in all `tf.variable_scope` blocks. This ensures that we don't recreate any variables but instead always reuse the original ones from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.set_random_seed(seed)\n",
    "\n",
    "# Number of sentences to generate\n",
    "num_to_generate = tf.placeholder(tf.int32, shape=(), name='NumToGenerate')\n",
    "\n",
    "# Select a random word to start each sentence\n",
    "random_start = tf.random_uniform(shape=(num_to_generate,), maxval=embedding_size, dtype=tf.int32)\n",
    "\n",
    "# Reuse ALL variables\n",
    "with tf.variable_scope('Softmax', reuse=True):\n",
    "    w = tf.get_variable(name='Weight')\n",
    "    b = tf.get_variable(name='Bias')\n",
    "\n",
    "def do_softmax(tensor):\n",
    "    \"\"\"Helper function to compute softmax.\"\"\"\n",
    "    scores = tf.matmul(tensor, w) + b\n",
    "    softmax = tf.nn.softmax(scores, name='Softmax')\n",
    "    return softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform the unrolling! Notice that this time the input to each word is the previous output of the RNN rather than some data that we provide it. This is the reason why we had to recreate the whole graph. We also have a new tensor called `generated` that just selects the word with the hightest probability from the `outputs` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Again, reuse ALL variables in this scope!\n",
    "with tf.variable_scope('Unrolled', reuse=True) as scope:\n",
    "    cell1 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "    cell2 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "    cell3 = tf.contrib.rnn.BasicLSTMCell(num_units=num_units)\n",
    "\n",
    "    # Feed into `initial_state` with `feed_dict` if you want to use the state of a prior sequence instead of zero\n",
    "    initial_state = state = cell1.zero_state(batch_size=num_to_generate, dtype=tf.float32)\n",
    "\n",
    "    # One-hot encode first word, and treat it as 1st output\n",
    "    prev_word = tf.one_hot(random_start, depth=embedding_size)\n",
    "\n",
    "    # Generate the sentence\n",
    "    outputs = [prev_word]  # python list of tensors so we can keep track of all the outputs\n",
    "    for i in range(sequence_length-1):  # We already \"made\" the first word, so generate `sequence_length-1` more\n",
    "        output, state = cell1(prev_word, state, scope='Cell1')  # Step the RNN through the sequence\n",
    "        output, state = cell2(output, state, scope='Cell2')  # 2nd layer\n",
    "        output, state = cell3(output, state, scope='Cell3')  # 3rd layer\n",
    "        output_word = do_softmax(output)  # \n",
    "        outputs.append(output_word)\n",
    "        prev_word = output_word\n",
    "\n",
    "    # Useful if you want longer outputs, you can fetch this tensor and then feed it back into `initial_state`\n",
    "    final_state = state\n",
    "    outputs = tf.stack(outputs, axis=1, name='Outputs')  # shape `[num_to_generate, sequence_length, embedding_size]`\n",
    "\n",
    "generated = tf.argmax(outputs, axis=-1, name='Generated')  # convert from one-hot encoding to index of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point we still havent told the architecture to compute anything. Lets do that now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"NumToGenerate:0\", shape=(), dtype=int32) is not an element of this graph.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/home/ryan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1067\u001b[0m             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n\u001b[0;32m-> 1068\u001b[0;31m                                                     allow_operation=False)\n\u001b[0m\u001b[1;32m   1069\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ryan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2707\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ryan/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2786\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2787\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tensor %s is not an element of this graph.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor Tensor(\"NumToGenerate:0\", shape=(), dtype=int32) is not an element of this graph.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ef669427d367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mnum_to_generate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0malice_load\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vocab'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ryan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ryan/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1069\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n\u001b[0;32m-> 1071\u001b[0;31m                             + e.args[0])\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"NumToGenerate:0\", shape=(), dtype=int32) is not an element of this graph."
     ]
    }
   ],
   "source": [
    "results = sess.run(generated, feed_dict={num_to_generate: 10})\n",
    "sentences = [[alice_load['vocab'][embedding] for embedding in sentence] for sentence in results]\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sentences are not that great, although some are direct sentences from the original novel. There are more advanced architectures that will give better results, such as seq2seq and more advanced types of RNN cells, although these are beyond the scope of this series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
