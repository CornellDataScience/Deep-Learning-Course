{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Whereas convolutional neural networks take advantage of spatial locality and are well-suited for images, recurrent neural networks take advantage of temporal locality and are well-suited for text. Most state-of-the-art applications of deep learning for NLP use a form of recurrent neural network.  \n",
    "  \n",
    "Here, we will create an RNN that is able to mimic the writing/speaking style from a passage of text. We will be using a character-level RNN to generate fake text - the network learns what characters are likely to follow after the sequence of characters that came before it. How do we train a network like this? Below is a sample of the training data we will be using: a transcript from various speeches by Donald Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People have asked me why I am running for President. I have built an amazing business that I love and I get to work side-by-side with my children every day. We come to work together and turn visions into reality. We think big, and then we make it happen. I love what I do, and I am grateful beyond words to the nation that has allowed me to do it. So when people ask me why I am running, I quickly answer: I am running to give back to this country which has been so good to me. When I see the crumbling roads and bridges, or the dilapidated airports, or the factories moving overseas to Mexico, or to other countries, I know these problems can all be fixed, but not by Hillary Clinton – only by me. T ...\n"
     ]
    }
   ],
   "source": [
    "with open('../data/trump.txt', 'r') as file:\n",
    "    transcript = file.read()\n",
    "transcript = transcript.replace('\\n', ' ')\n",
    "print(transcript[:700], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our RNN to predict, given the sequence of characters that came before it, what should come next. So given \"People have asked me wh\", our model should then predict \"y\". Should we train by treating the entire piece of text as one long string as above? This is probably a bad idea - our model will overfit to the exact word usage in the training data, perhaps learning sequences of sentences at a time.   \n",
    "  \n",
    "Instead we will break up the text into small chunks, so that our network learns \"general\" word usage - to predict characters that make sense given only the *immediately* preceding text. For this reason, we may find that RNN-generated text lacks any coherent direction and appears to just ramble on about a certain topic (sound familiar?).   \n",
    "  \n",
    "So how should we break up our text? We want to break it up such that the chunks represent the temporal locality that the RNN should take into account, i.e. how far back does it need to look to predict the next character. It might be intuitive to use sentences as our segments, but this presents computational difficulties because not all sentences are the same length (more on this later!) - here, we choose to create a window of fixed length, and slide it through our text one character at a time (much like a convolutional filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People have asked me why I am running for Presiden    t\n",
      "eople have asked me why I am running for President    .\n",
      "ople have asked me why I am running for President.     \n",
      "ple have asked me why I am running for President.     I\n",
      "le have asked me why I am running for President. I     \n",
      "e have asked me why I am running for President. I     h\n",
      " have asked me why I am running for President. I h    a\n",
      "have asked me why I am running for President. I ha    v\n",
      "ave asked me why I am running for President. I hav    e\n",
      "ve asked me why I am running for President. I have     \n"
     ]
    }
   ],
   "source": [
    "num_steps = 50  # size of \"unrolling\" - more on this later!\n",
    "for i in range(10):\n",
    "    print('{}    {}'.format(transcript[i: i + num_steps], transcript[i + num_steps]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all other types of neural networks, these are algorithms that work strictly with numbers, so we have to encode our text in vector form. Here, we will use a simple one-hot encoding, but will implement it so that the encoding we choose can flexibly be changed. First, let's take a look at the dimension of our one-hot encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: [' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '–', '—', '‘', '’', '“', '”', '…']\n",
      "Number of unique characters: 91\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(transcript)))\n",
    "num_classes = len(unique_chars)\n",
    "char2idx = dict(zip(unique_chars, range(num_classes)))\n",
    "\n",
    "print('Unique characters:', unique_chars)\n",
    "print('Number of unique characters:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "The input to our RNN is a sequence of numbers, where each number represents a character in our sequence. The output is the same sequence, but shifted over by one character: this way, our RNN learns to generate the next character, while the hidden states summarize the sequential information that came before it. The diagram shows an LSTM cell (a variant of the basic RNN cell we implement below), but the structure of the network is exactly the same - at every time step, the RNN updates its internal/hidden state based on the current input and the previous internal/hidden state, and produces an output based on the internal/hidden state.\n",
    "\n",
    "<img src=\"../images/RNN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/RNN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "# encode every character as an integer\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_embeddings = tf.eye(num_classes)\n",
    "x_embed = tf.nn.embedding_lookup(one_hot_embeddings, x)\n",
    "y_embed = tf.nn.embedding_lookup(one_hot_embeddings, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/BasicRNNLabeled.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs list: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack:0' shape=(25, 91) dtype=float32>,\n",
       " <tf.Tensor 'unstack:1' shape=(25, 91) dtype=float32>,\n",
       " <tf.Tensor 'unstack:2' shape=(25, 91) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_inputs = tf.unstack(x_embed, axis=1)\n",
    "y_outputs = tf.unstack(y_embed, axis=1)\n",
    "\n",
    "print('Length of inputs list:', len(x_inputs))\n",
    "x_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 128\n",
    "\n",
    "with tf.variable_scope('rnn_cell_1'):\n",
    "    W_xh = tf.get_variable('W_xh', [num_classes, state_size])\n",
    "    W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "    b_h = tf.get_variable('b_h', [state_size])\n",
    "    \n",
    "with tf.variable_scope('rnn_cell_2'):\n",
    "    W_xh = tf.get_variable('W_xh', [state_size, state_size])\n",
    "    W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "    b_h = tf.get_variable('b_h', [state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, prev_state, scope):\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_xh = tf.get_variable('W_xh', [rnn_input.get_shape().as_list()[1], state_size])\n",
    "        W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "        b_h = tf.get_variable('b_h', [state_size])\n",
    "        return tf.tanh(tf.matmul(prev_state, W_hh) + tf.nn.xw_plus_b(rnn_input, W_xh, b_h)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([batch_size, state_size])\n",
    "state1 = init_state\n",
    "hidden_states_1 = []\n",
    "\n",
    "for rnn_input in x_inputs:\n",
    "    state1 = rnn_cell(rnn_input, state1, 'rnn_cell_1')\n",
    "    hidden_states_1.append(state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2 = init_state\n",
    "hidden_states_2 = []\n",
    "\n",
    "for rnn_input in hidden_states_1:\n",
    "    state2 = rnn_cell(rnn_input, state2, 'rnn_cell_2')\n",
    "    hidden_states_2.append(state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W_hy = tf.get_variable('W_hy', [state_size, num_classes])\n",
    "    b_y = tf.get_variable('b_y', [num_classes])\n",
    "    \n",
    "logits = [tf.nn.xw_plus_b(state, W_hy, b_y) for state in hidden_states_2]\n",
    "preds = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit)\n",
    "          for label, logit in zip(y_outputs, logits)]\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the total loss, we simply average the loss across each of the time steps. Then we define a `train_step` as usual, specifying a learning rate hyperparameter - do not underestimate the difficulty of hyperparameter optimization! It is an area of ongoing research with very few theoretical results, and is usually the most time-consuming part of any machine learning project.\n",
    "\n",
    "`zip` is a built-in Python function that creates a list of tuples from two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'b'), (3, 'c')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "list(zip(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ([], [])\n",
    "print('Pre-processing training data...')\n",
    "t0 = time.time()\n",
    "\n",
    "for i in range(len(transcript) - num_steps - 2):\n",
    "    x_text = transcript[i: i + num_steps]\n",
    "    y_text = transcript[i + 1: i + num_steps + 1]\n",
    "    data[0].append([char2idx[char] for char in x_text])\n",
    "    data[1].append([char2idx[char] for char in y_text])\n",
    "\n",
    "print('Pre-processed {} characters in {:.2f}s'.format(len(transcript), time.time() - t0))\n",
    "data = (np.array(data[0], dtype=int), np.array(data[1], dtype=int))\n",
    "data = np.hstack(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 100\n",
    "\n",
    "for i in range(max_epochs):\n",
    "    np.random.shuffle(data_train)\n",
    "    for j in range(data_train.shape[0] // batch_size):\n",
    "        start, end = j * batch_size, (j + 1) * batch_size\n",
    "        x_batch, y_batch = data_train[start:end, :num_steps], data_train[start:end, num_steps:]\n",
    "        sess.run(train_step, feed_dict={x: x_batch, y: y_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from rnn_model import RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../models/rnn/'\n",
    "\n",
    "with open(os.path.join(save_dir, 'config.pkl'), 'rb') as f:\n",
    "    saved_args = pickle.load(f)\n",
    "with open(os.path.join(save_dir, 'chars_vocab.pkl'), 'rb') as f:\n",
    "    chars, vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "model = RNN(saved_args, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../models/rnn/model.ckpt-25049\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "ckpt = tf.train.get_checkpoint_state(save_dir)\n",
    "saver.restore(sess, ckpt.model_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000         # length of string to generate\n",
    "prompt = 'The '  # prompt for RNN to start generating from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The or comes and borders, no future on what we send me for the country and eviduty. Suponal Marco - these are their great new nementrative. The inner country.\n",
      "And the drugs liking our problems.\n",
      "It’s ridiculous, it’s been changed that I raison the president of country.\n",
      "We are going to instruct congent - they plad. We're going to take care of something were elect of our mines we're doing.\n",
      "Get ritting for the pack, and they have very well, those good partons. Just surprised. And I want certainly executive—they have a nice – with the smartest, a factapo show to do it and met for world's happening will: We accurit it on them and we're going to make sure years and they're being—we should say, \"Cousm are into us. And the ones in the world learned a couple old our AbPural richment problem. We've got to lose up and this that a little stripped of it. I guess for you. I love Chicago. I do after the world is being. All over Will and it can't approve. You saw Honsly destroyed and is a disaster. We have\n"
     ]
    }
   ],
   "source": [
    "output = model.sample(sess, chars, vocab, n, prompt)\n",
    "output.replace(\"\\'\", \"'\")\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
