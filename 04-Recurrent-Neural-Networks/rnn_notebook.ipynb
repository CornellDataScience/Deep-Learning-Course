{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data\n",
    "\n",
    "Whereas convolutional neural networks take advantage of spatial locality and are well-suited for images, recurrent neural networks take advantage of temporal locality and are well-suited for text. Most state-of-the-art applications of deep learning for NLP use a form of recurrent neural network.  \n",
    "  \n",
    "Here, we will create an RNN that is able to mimic the writing/speaking style from a passage of text. We will be using a character-level RNN to generate fake text - the network learns what characters are likely to follow after the sequence of characters that came before it. How do we train a network like this? Below is a sample of the training data we will be using: a transcript from various speeches by Donald Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People have asked me why I am running for President. I have built an amazing business that I love and I get to work side-by-side with my children every day. We come to work together and turn visions into reality. We think big, and then we make it happen. I love what I do, and I am grateful beyond words to the nation that has allowed me to do it. So when people ask me why I am running, I quickly answer: I am running to give back to this country which has been so good to me. When I see the crumbling roads and bridges, or the dilapidated airports, or the factories moving overseas to Mexico, or to other countries, I know these problems can all be fixed, but not by Hillary Clinton – only by me. T ...\n"
     ]
    }
   ],
   "source": [
    "with open('../data/trump.txt', 'r') as file:\n",
    "    transcript = file.read()\n",
    "transcript = transcript.replace('\\n', ' ')\n",
    "print(transcript[:700], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our RNN to predict, given the sequence of characters that came before it, what should come next. So given \"People have asked me wh\", our model should then predict \"y\". Should we train by treating the entire piece of text as one long string as above? This is probably a bad idea - our model will overfit to the exact word usage in the training data, perhaps learning sequences of sentences at a time.   \n",
    "  \n",
    "Instead we will break up the text into small chunks, so that our network learns \"general\" word usage - to predict characters that make sense given only the *immediately* preceding text. For this reason, we may find that RNN-generated text lacks any coherent direction and appears to just ramble on about a certain topic (sound familiar?).   \n",
    "  \n",
    "So how should we break up our text? We want to break it up such that the chunks represent the temporal locality that the RNN should take into account, i.e. how far back does it need to look to predict the next character. It might be intuitive to use sentences as our segments, but this presents computational difficulties because not all sentences are the same length (more on this later!) - here, we choose to create a window of fixed length, and slide it through our text one character at a time (much like a convolutional filter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People have asked me why I am running for Presiden    t\n",
      "eople have asked me why I am running for President    .\n",
      "ople have asked me why I am running for President.     \n",
      "ple have asked me why I am running for President.     I\n",
      "le have asked me why I am running for President. I     \n",
      "e have asked me why I am running for President. I     h\n",
      " have asked me why I am running for President. I h    a\n",
      "have asked me why I am running for President. I ha    v\n",
      "ave asked me why I am running for President. I hav    e\n",
      "ve asked me why I am running for President. I have     \n"
     ]
    }
   ],
   "source": [
    "num_steps = 50  # size of \"unrolling\" - more on this later!\n",
    "for i in range(10):\n",
    "    print('{}    {}'.format(transcript[i: i + num_steps], transcript[i + num_steps]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with all other types of neural networks, these are algorithms that work strictly with numbers, so we have to encode our text in vector form. Here, we will use a simple one-hot encoding, but will implement it so that the encoding we choose can flexibly be changed. First, let's take a look at the dimension of our one-hot encoded vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: [' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'é', '–', '—', '‘', '’', '“', '”', '…']\n",
      "Number of unique characters: 91\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(transcript)))\n",
    "num_classes = len(unique_chars)\n",
    "\n",
    "print('Unique characters:', unique_chars)\n",
    "print('Number of unique characters:', num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Overview\n",
    "\n",
    "TODO: explain\n",
    "\n",
    "<img src=\"../images/RNN.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "\n",
    "# encode every character as an integer\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_embeddings = tf.eye(num_classes)\n",
    "x_embed = tf.nn.embedding_lookup(one_hot_embeddings, x)\n",
    "y_embed = tf.nn.embedding_lookup(one_hot_embeddings, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/BasicRNNLabeled.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of inputs list: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'unstack:0' shape=(25, 91) dtype=float32>,\n",
       " <tf.Tensor 'unstack:1' shape=(25, 91) dtype=float32>,\n",
       " <tf.Tensor 'unstack:2' shape=(25, 91) dtype=float32>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_inputs = tf.unstack(x_embed, axis=1)\n",
    "y_outputs = tf.unstack(y_embed, axis=1)\n",
    "\n",
    "print('Length of inputs list:', len(x_inputs))\n",
    "x_inputs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 128\n",
    "\n",
    "with tf.variable_scope('rnn_cell_1'):\n",
    "    W_xh = tf.get_variable('W_xh', [num_classes, state_size])\n",
    "    W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "    b_h = tf.get_variable('b_h', [state_size])\n",
    "    \n",
    "with tf.variable_scope('rnn_cell_2'):\n",
    "    W_xh = tf.get_variable('W_xh', [state_size, state_size])\n",
    "    W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "    b_h = tf.get_variable('b_h', [state_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, prev_state, scope):\n",
    "    with tf.variable_scope(scope, reuse=True):\n",
    "        W_xh = tf.get_variable('W_xh', [rnn_input.get_shape().as_list()[1], state_size])\n",
    "        W_hh = tf.get_variable('W_hh', [state_size, state_size])\n",
    "        b_h = tf.get_variable('b_h', [state_size])\n",
    "        return tf.tanh(tf.matmul(prev_state, W_hh) + tf.nn.xw_plus_b(rnn_input, W_xh, b_h)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_state = tf.zeros([batch_size, state_size])\n",
    "state1 = init_state\n",
    "hidden_states_1 = []\n",
    "\n",
    "for rnn_input in x_inputs:\n",
    "    state1 = rnn_cell(rnn_input, state1, 'rnn_cell_1')\n",
    "    hidden_states_1.append(state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state2 = init_state\n",
    "hidden_states_2 = []\n",
    "\n",
    "for rnn_input in hidden_states_1:\n",
    "    state2 = rnn_cell(rnn_input, state2, 'rnn_cell_2')\n",
    "    hidden_states_2.append(state2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W_hy = tf.get_variable('W_hy', [state_size, num_classes])\n",
    "    b_y = tf.get_variable('b_y', [num_classes])\n",
    "    \n",
    "logits = [tf.nn.xw_plus_b(state, W_hy, b_y) for state in hidden_states_2]\n",
    "preds = [tf.nn.softmax(logit) for logit in logits]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit)\n",
    "          for label, logit in zip(y_outputs, logits)]\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the total loss, we simply average the loss across each of the time steps. Then we define a `train_step` as usual, specifying a learning rate hyperparameter - do not underestimate the difficulty of hyperparameter optimization! It is an area of ongoing research with very few theoretical results, and is usually the most time-consuming part of any machine learning project.\n",
    "\n",
    "`zip` is a built-in Python function that creates a list of tuples from two lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'b'), (3, 'c')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "b = ['a', 'b', 'c']\n",
    "list(zip(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
