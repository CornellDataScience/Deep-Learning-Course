{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is for compatibility with Python 2. If you are using Python 3 (recommended), you may ignore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must import the TensorFlow and numpy packages to be able to use them! We use the prefix \"tf\" to avoid having to type out the full name every time we want to use a TensorFlow command. Likewise, we prefix all numpy commands with \"np\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "tf.set_random_seed(seed)  # Tell TensorFlow to use our seed\n",
    "np.random.seed(seed)      # Tell NumPy to use our seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        if sys.version[0] == '3':\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        elif sys.version[0] == '2':\n",
    "            dict = pickle.load(fo)\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper function that will allow us to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = unpickle('../cifar-10-data')\n",
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the data is stored in a dictionary (a data structure in Python). We are interested in the labels and the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[b'data'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 10000 images in this \"batch\", and each is stored in an array of length 3072. Why is this? Hint: The images are 32x32  \n",
    "Let's plot a random image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHKlJREFUeJztnWtsnNeZ3//PXHgZkqJEkZJpXSzLKye2E1tOtG62SbZp\ng2S9wRZJgMJIPmz9IVgt0G3aALsfjBRoUhQF0qLJIh+KAEpjrLfI5oJcNkYRJOt4kzjp7jqmE1uW\nrU1sObpfKEokxctwrk8/zAiV2fM/HA3Jl1TO/wcIGp5nzpzznnmfeWfO/32ex9wdQoj0yG30BIQQ\nG4OcX4hEkfMLkShyfiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiRKYTWdzexhAJ8HkAfwP939M7Hn\nD20b9bFde4O2bu40NOO22KsZIh3F5uQWvxM1Nnsn1ugRE+OV82cwP3OloxO8a+c3szyA/wHgfQDO\nAnjOzJ5091dYn7Fde/Gfv/GToK3ZqHczB2rr1vljHyi0z619Xt4S3BK3oUem2PQmtdUQttWbvA9q\n4cH+y79+H++zjNV87X8IwGvu/rq7VwF8FcAHV/F6QogMWY3z7wJw5oa/z7bbhBC3AOu+4Wdmh81s\nwswmrk1PrfdwQogOWY3znwOw54a/d7fb3oC7H3H3Q+5+aMu20VUMJ4RYS1bj/M8BOGBmd5pZD4CP\nAHhybaYlhFhvut7td/e6mf1bAN9HS+p73N1fjvUxAwr58HZ6s5vPoW625oG40Bd5TWbJRTZlu5/I\nLU4Xm/NM8moZu1us6Gt2QUwpio3lzs/vHFEy8pFjbpKXu5lVWpXO7+7fBfDd1byGEGJj0B1+QiSK\nnF+IRJHzC5Eocn4hEkXOL0SirGq3v6sBc0Tqi8gasQCezYB1+xG6uQ9rVay1CNu1YEfPq7UPFPLY\nUUeGMxLTZpFgpibxo5s5FXXlFyJR5PxCJIqcX4hEkfMLkShyfiESJdPdfgOQZwEJ3SQs2yToE3QT\ns0kUlWZs574e3u7PNRq0TyuL3urQeStEosj5hUgUOb8QiSLnFyJR5PxCJIqcX4hEyTawxww5EgXj\nFpE1qNQX03HWXuOhOfyi8RyxeXQpYa6xLNptXrruyFZ76yomLF7uKdItksOvyc/vRrUSbK9VeBUr\nK/SEx7mJyka68guRKHJ+IRJFzi9Eosj5hUgUOb8QiSLnFyJRViX1mdlJAHMAGgDq7n5oxU65cDSS\nO5dCCiCSR7S609p/rrHIw1hZpUZEa2quQ7SiIVw7LC6Kdpc7LzZ/lncxlo8xFtm55gJhZKxc5Fzk\n4hsAUooOAHIRKbtWmQ+2V5f4UL19YanvZk6ptdD5/7m7q/a2ELcY+tovRKKs1vkdwA/M7HkzO7wW\nExJCZMNqv/a/y93PmdkOAE+Z2T+6+zM3PqH9oXAYAMZ27V3lcEKItWJVV353P9f+fxLAtwE8FHjO\nEXc/5O6HtoyMrmY4IcQa0rXzm9mAmQ1dfwzg/QCOrdXEhBDry2q+9u8E8O22dFMA8Ffu/r1YBwMv\nQWSRzyHzm/+MWo+Un0zJqc7P0T4WkX96+vuprRGJzorJmN5FGFu3kXu5zbJf3EUUXrexlvGSXJH3\nxYk0B6C8MBtsX1os0z69RSb1haXeEF07v7u/DuCBbvsLITaWTfLRLYTIGjm/EIki5xciUeT8QiSK\nnF+IRMm4Vp+jiGrQ1mzyqfC6ZDxSKheRPGJSTi7HPw9nL18Ktj/97W/QPkODg9R295vfRG3924ap\nbWBsjNpKgyPB9kYk8tCNr1Xs6hCXYMkqd6nBRq9SXYT8xSTRRuQciB1ALibPOj+/p69cCLafPMFv\nm/mnv/P7ZCDuE8vRlV+IRJHzC5Eocn4hEkXOL0SiyPmFSJRsy3V5A7nmtfBEjO9us31Zlq8OWKF0\nUmRXNm9FapuZuhhsP/oPP+JjLYXVDQD49dE91LZl105q2/fW+6ntd979e8F2sz7apxHZ7WeBWEB8\nd5sTyfsX2baPb+jH+oXHi+32xwKnGtUFart0/jy17dzB3+tGNRzYc/K1X9A+W0oDwfZymQeZLUdX\nfiESRc4vRKLI+YVIFDm/EIki5xciUeT8QiRKplJfrbaE82deCdrG9/w27dckQTpMxmnZuvtc8wYv\nyNSoV4Ltw72RMk0NPseFybPUduVaONgDAC7PXKa2/sKWYPv9b3sn7ZPrjciikeApW+PTJxfR8+I5\n92J124jU1+SvmC/wc+fsqV9S2z/8+PvU9tBD76K20ydeDrZfPn+K9nluMXwuLixI6hNCrICcX4hE\nkfMLkShyfiESRc4vRKLI+YVIlBW1GjN7HMAfAJh097e020YAfA3APgAnATzi7tMrvVZlaREnfvVS\n0Hb7Ll78J8ci7SKRWTFpqJnnn3n1pbCEAgC/evH5YHuutkj77Ijk8Ds5yeU8WDhqCwCas+HISAD4\n2yf/Otg+UOSvd++Db6W2ekx+i2hzLGVgo8lluUYk/1whklfPIlF4OWLLR+TBeoWv7y9f+Htqe+UX\nP6G2+dlz1Hb+9Olg+8wsd6laM7xWjTqPIl1OJ1f+vwDw8LK2xwA87e4HADzd/lsIcQuxovO7+zMA\nri5r/iCAJ9qPnwDwoTWelxBinen2N/9Od7/+nfUiWhV7hRC3EKve8PNWWhz6A8rMDpvZhJlNLMzx\nLChCiGzp1vkvmdk4ALT/n2RPdPcj7n7I3Q8NDPFNJyFEtnTr/E8CeLT9+FEA31mb6QghsqITqe8r\nAN4DYNTMzgL4FIDPAPi6mX0MwCkAj3QyWKNex+xU+EtCY4nLK4X+HcH2Js87CTMueXiOJ+m8SuYH\nACeOPhdsH+rhyzjc20ttV6Z4dF59dobaRhb5gW8bDWtsv5z4Ke3z+vEXqW1w6zZqe+Dtb6O2Yn84\nYWgzVgorIisyaQsAKmX+Xpfn5oPt8zNXaJ8zp8JRdgDwygSX85qR5JmT505S2xyZY99AifbJFcg5\ncBOly1Z0fnf/KDG9t/NhhBCbDd3hJ0SiyPmFSBQ5vxCJIucXIlHk/EIkSqYJPOv1Kq5eCSet/PXr\nR2m/N9337mC75fppn2Ik0isfqTF35uRJapuZCctve8dHaR8s1KgpVuoulki0vBCu7QYA20bC0lxl\nlkuYx577GbX19PB1nH6NS4R9A+EbuvoH+XuGSMTfzGUuzZUjd46eJRFz83ORRJc9kcjDOo/gzEVq\nHtZz/P0c7B0KtpcjyV+bzXLYcBP1E3XlFyJR5PxCJIqcX4hEkfMLkShyfiESRc4vRKJkKvV5s4Fq\nOZyU8Py5cA0/ADjwpoPB9oV5IncAqEekrVykFtv81CVqq1TDyT0rkYiz6UiU4OxiOJoLAEolnvug\nUIgkLvVwhFsjIg+ODfAox3yTJzSdPhFOxgoAlXJYEqvX+OvFVKr+AZ4IdWSIR781r7wenscijwQ8\n8Ob7qK2vJxxhCgDz5JgB4NTl5Znw/h8ztfB5YANcHuwbIufwTUT16covRKLI+YVIFDm/EIki5xci\nUeT8QiRKprv9zWYDVZLn7PSved601189HmzvzY/RPq/97EfUNtTPd7dzNb7DWidBHc8e/QXtMzbI\nc+CVI+WpGvNcCRjdwY+7UQvvYi/M85yA2yN5+hrVyPZxNZJEsRxex1KOb+kX+nqobXzfbdSWr/PA\nnnN94cCqaxUecNWsciVgaJCrMLtHt1PbyNBWavvq954Ktu84wJWFrbuGg+2FfJ72WY6u/EIkipxf\niESR8wuRKHJ+IRJFzi9Eosj5hUiUTsp1PQ7gDwBMuvtb2m2fBvBHAK7Xm/qku393xdcCkCPRGzNX\nL9J+F8+fC7a/++330j73vOed1HbiFZ57bv7cFLUVcmFpbgZcHhzu5dLL+F13UNuZ4yeorbLExyuO\nhMuDFXvD5bMAwCOBQtU6n7/18ICaCsLl1/INLrH15bnUN9jDy57lwYOFxraGJbHLczwn4NRMOPgM\nAKwRCUyq8JJz49u5PDvcFz62yiIfq5/0Mes8sqeTK/9fAHg40P7n7n6w/W9FxxdCbC5WdH53fwYA\nj0cUQtySrOY3/8fN7KiZPW5m/BYxIcSmpFvn/wKA/QAOArgA4LPsiWZ22MwmzGyissR/7wkhsqUr\n53f3S+7ecPcmgC8CeCjy3CPufsjdD/X28XvqhRDZ0pXzm9n4DX9+GMCxtZmOECIrOpH6vgLgPQBG\nzewsgE8BeI+ZHQTgAE4C+ONOBnM3NKph6ahiXFLKF8PTrLOSRQB6IhFiW0r8sMcHedTZnWNhaauv\nP1I2bGgvtT1wcJzamkv8c7m6tERthVy4n5NoPwCYmuF5Bi9M8b3eUonn1et18hOvwt+zvhp/z2av\nXqY2q/Hceb3F8HtTrfKfoItVHiWIAo/qm57mMvF8RMrusfBccv18rC3bw8eVj+SnXM6Kzu/uHw00\nf6njEYQQmxLd4SdEosj5hUgUOb8QiSLnFyJR5PxCJEqmCTwBgyMs5ywucImtvBRO+jk5dYr2KZCo\nJwDoG+TS3IP37Ke2C+fCSUYvHz1N++z5LS7n3TE+Sm35+/k8Jv7uWWqbmw3LTYVI+a9GmUejTV86\nT21TkdNnmCRJ7Svw93mgxKW+mQU+x/Jc+PwAgAUSALkQSdJZX+Rj1cGj8/r6+Hm1cCUcmQoAjXpY\n/hzespP26R8MR+8RpTf83M6fKoT4TULOL0SiyPmFSBQ5vxCJIucXIlHk/EIkSqZSX19/Hw7cd3fQ\nNj3DI7PKs5eC7ceO8iiqn03ySLVimUeW/dm/+zfU9uEtYbls6/Yf0z4LUxeobWDyVWq7e5BH7p3g\nuThx9nRY/szv2Uf71Opcfqs4vz7MX+MSW3khLEUNxuok5vmBzS3ypKVXZ/h5sECi92YW+Pr28KFw\n4tRZatuzPZwsFACKRR61WmmEax4WcryP19kk+Xu5HF35hUgUOb8QiSLnFyJR5PxCJIqcX4hEyXS3\nP1/IY/ttI0Hbjp08YALN8M7xtVleVunyNb7LPneO9zt9gasEt4/eHmx//z97L+1z5sXnqe3qeV42\nLDe2ldrGR3mZhNdOHA+218Mbyi0beImn+YgyYpF8cVWy6zxb5iWoypf4rn3e+FhzlVlqK5RIWauI\n6jAdUTEW5vl6VMo899/tYzzf4WItXAaut58HOrFcfRZ5L5ejK78QiSLnFyJR5PxCJIqcX4hEkfML\nkShyfiESpZNyXXsA/CWAnWhFDRxx98+b2QiArwHYh1bJrkfcnWtoAGAOWDggwcFzqrmFpRCWxwwA\ndu7aQW39uXDZLQCoNcNjAcA8kRbNuWz02+/7V9T26ss8R1ulxiWxnud47sJ+kp/Qja/VzOwMtdWb\nkSgXiwSROLGxdgCFGi+hZTk+//7RSE7Gf3J/sH1shOdP/NHf8ByJF8/wsmHnrvJjm1/i72ctHz62\nge38PG2SmB/vXOnr6MpfB/Cn7n4vgHcA+BMzuxfAYwCedvcDAJ5u/y2EuEVY0fnd/YK7/7z9eA7A\ncQC7AHwQwBPtpz0B4EPrNUkhxNpzU7/5zWwfgAcBPAtgp7tfv43uIlo/C4QQtwgdO7+ZDQL4JoBP\nuPsbEpu7u4NkETCzw2Y2YWYTC3M8YYcQIls6cn4zK6Ll+F9292+1my+Z2XjbPg4geFO8ux9x90Pu\nfmhgiG9gCCGyZUXnNzMD8CUAx939czeYngTwaPvxowC+s/bTE0KsF51E9b0TwB8CeMnMXmi3fRLA\nZwB83cw+BuAUgEdWeiFzIEeko2qDSyHF3vBn1OLCPO1Tdx7Glu/j0VJ//eS3qO3B/eFtjclJHlW2\n4553U1v/Nr5NMvF3f0ttp6d49FtpKJxnsFLh6zFQ4rnz6uBS3/ad26ktlw9rUfkCl0V7SB8A2LXr\nNmrbfR+3jY5vCbb3Gj/1Z2Z4VN/3J39CbTWmvwGYq3ANbscd4fnv2BuOgAUA6yHS+E1IfSs6v7v/\nNPKSPJZVCLGp0R1+QiSKnF+IRJHzC5Eocn4hEkXOL0SiZJrAs9FsYH4xLKMsLvG7/4woKPMLPJki\nnB9ao8jlq+899UNqu3A8nMBzMpLUsfnyCWqLyWiVSFLKnhEexVa9GI48XJzn0Ypl5/MYi8hN//Ij\n76c26wsLRLl8ZO5zfB63RZKWlvPXuK0WloNL/fyGswP33EVt/+fHz1FbZS5SiqyPH/fd970p2L5j\nhK99uRb2ozxzltCcOn6mEOI3Cjm/EIki5xciUeT8QiSKnF+IRJHzC5EomUp9ZoZCMTykL/KoM5ZT\n0yL124p93Nbfz2WXA2+5m9r2j+wKtueu8fp+MzmemHTndp5EsrT9TmqrLS5R2/T5sAQ0dzWWpJMn\nnpyd5ZGTc0u8Nl2eBE5Wq1yWswaXyi7Nchmw3sPXgylf0xGZuFHg61GK5KSYneTr0YjUSpyeCr83\nXgufbwCQb7AMnnyc5ejKL0SiyPmFSBQ5vxCJIucXIlHk/EIkSqa7/e5N1CvhXH2DkUCLQiE8zaVI\nKalGjQey5HL8sLdFAkjmyuGd6rse2MvnsYUrC705HoQxvch32YulYWobvj1cpuz8SR4otGcHz4F3\nYfYit52/Qm1jvYPB9mYkmGl4mJ8D+Ty/ThVK4bEAoOHh86C3h49V7Oultt137aa2cyd+RW1o8vmf\nPX0h2F6uvJn2KQ6E52i5zq/nuvILkShyfiESRc4vRKLI+YVIFDm/EIki5xciUVaU+sxsD4C/RKsE\ntwM44u6fN7NPA/gjAJfbT/2ku393pddjcQelEpdeWADP/DwPVjHwSIpCD5dySlvC5a4AYGRruKxV\nKRKgMwMe2FOrRUqKFXkJrTkilwLA9t1hqa849Gva54EHwjnkAKB6lI9Vq/L5j24Pl/LyfI32KfXw\nta81eMRKs8gDggpEImwVlg7TF8m391v37Ke2l589Q22DJX5s7FxtOL82b90alntZmbQQnej8dQB/\n6u4/N7MhAM+b2VNt25+7+3/veDQhxKahk1p9FwBcaD+eM7PjAHisoRDiluCmfvOb2T4ADwJ4tt30\ncTM7amaPmxm/NU4Iseno2PnNbBDANwF8wt2vAfgCgP0ADqL1zeCzpN9hM5sws4nFeZ50QQiRLR05\nv5kV0XL8L7v7twDA3S+5e8PdmwC+COChUF93P+Luh9z9UGmQb2IJIbJlRec3MwPwJQDH3f1zN7SP\n3/C0DwM4tvbTE0KsF53s9r8TwB8CeMnMXmi3fRLAR83sIFrq3UkAf7zSCzmAOvm4aeTC5Z0AoFAI\nyxc9vVziqSzwfGp9Jf4NZGRHWKICgD6ieuWLXDr0SHRhf0RSykciFms1btu9Lxyhd3IflyOHd/L1\nuO8BntOwNMDnP7RlS7B9cSmcYxAAqlX+s7ARWQ/LhccCgAaRCMsLPMqxFHlf+gdJckIAt9/J13jv\nHXyP/PzZcOTk5anIHG8LS4fNiIS5nE52+38KIOSZK2r6QojNi+7wEyJR5PxCJIqcX4hEkfMLkShy\nfiESJdtyXbkc8v1hqWSxwaPfegthGXBwmEs8+UjdolqDR5ZZkX8eLs6FZaqBJpd/IrkggRqXtnLO\nI+Z2jPAEnvVSWBa97+1csmOltQBg/7Y91Hb6Mk/uOTs9HWwv9vLBapFoxXqDr1WpNyL11cNS61B/\nJMousvYDJHEmAOy6a4za9h4IR1sCwDUiO167xmXRxXK43FizGakLtgxd+YVIFDm/EIki5xciUeT8\nQiSKnF+IRJHzC5EomUp9MCBHAvEqS1zqqy+GpblGJKov38cPzXKxxJk8AWKhtDXYvlTn0mFPJOLP\niIQJAPkGtxXZIgKwYljivPutd9I+aPDIQ9T5PBadR04aSe45vIUnar2yGJavAKBW5dJtLjL/fCMc\nDVjMx059PlYsknFgmMuYozu5PLtrz0iwvVLj0mcveVuMv13/H7ryC5Eocn4hEkXOL0SiyPmFSBQ5\nvxCJIucXIlGylfrggIelF7NIFF493KdSjUg8+VhCUH7YDeMyYI3UDKzWuNRXJnMHgEYjFj3GJbFa\nZLwCqdXWO8Qlx2gkWJ3bdu8PJwsFgD4SvRlRKdE/wBOJFiPhkeXFeWqrk/Uv5HhUXy5yDuTy/ABu\nu50nfy2V+Pz33xWOnJy8fDnYDgC9JPo0dxNan678QiSKnF+IRJHzC5Eocn4hEkXOL0SirLjbb2Z9\nAJ4B0Nt+/jfc/VNmNgLgawD2oVWu6xF3Dyduu447GiQIxklZJQBAM7yrX44EAyEXCQQhu/YAkMtx\nW50EkMyXeX652M58JH4EQ0uD1DZY4jvVA6WwSlAo8F3qpVgASQ/vVyNBMwDQaIaPO8e7oH8oEjRj\nPGhmqcxPY7b+uUh5uJ4erjpYxGX23slLcjUiwUf9Q+H3bLyPqynId56rj9HJlb8C4F+4+wNoleN+\n2MzeAeAxAE+7+wEAT7f/FkLcIqzo/N7iupBabP9zAB8E8ES7/QkAH1qXGQoh1oWOfvObWb5doXcS\nwFPu/iyAne5+of2UiwB2rtMchRDrQEfO7+4Ndz8IYDeAh8zsLcvsDvIL1swOm9mEmU0szvHfxkKI\nbLmp3X53nwHwQwAPA7hkZuMA0P5/kvQ54u6H3P1QaYhvpAghsmVF5zezMTPb2n7cD+B9AP4RwJMA\nHm0/7VEA31mvSQoh1p5OAnvGATxhZnm0Piy+7u7/28z+HsDXzexjAE4BeGTll3JYkwRaGM+dxxKT\nTU1f5X0igT1DW2Jlvvjn4ZXpmWD73AL/ORMLIioWuXx1bZ7nx/NIIE6tHpY/twzzHHJL1UiZLCLZ\ntWxcanUSUNPTx6XD3khuxd4efn54k9tyRBKLBVXFjtkROWbwc64aCZBiwUKFIj936iDv2U3k8FvR\n+d39KIAHA+1XALy386GEEJsJ3eEnRKLI+YVIFDm/EIki5xciUeT8QiSKtW7Oy2gws8toyYIAMApg\nKrPBOZrHG9E83sitNo873H2skxfM1PnfMLDZhLsf2pDBNQ/NQ/PQ134hUkXOL0SibKTzH9nAsW9E\n83gjmscb+Y2dx4b95hdCbCz62i9EomyI85vZw2b2SzN7zcw2LPefmZ00s5fM7AUzm8hw3MfNbNLM\njt3QNmJmT5nZq+3/t23QPD5tZufaa/KCmX0gg3nsMbMfmtkrZvaymf37dnumaxKZR6ZrYmZ9ZvYz\nM3uxPY//1G5f2/Vw90z/AcgDOAFgP4AeAC8CuDfrebTnchLA6AaM+7sA3gbg2A1t/w3AY+3HjwH4\nrxs0j08D+LOM12McwNvaj4cA/ArAvVmvSWQema4JWoG5g+3HRQDPAnjHWq/HRlz5HwLwmru/7u5V\nAF9FKxloMrj7MwCWJyPIPCEqmUfmuPsFd/95+/EcgOMAdiHjNYnMI1O8xbonzd0I598F4MwNf5/F\nBixwGwfwAzN73swOb9AcrrOZEqJ+3MyOtn8WrPvPjxsxs31o5Y/Y0CSxy+YBZLwmWSTNTX3D713e\nSkz6+wD+xMx+d6MnBMQTombAF9D6SXYQwAUAn81qYDMbBPBNAJ9w92s32rJck8A8Ml8TX0XS3E7Z\nCOc/B+DGguS7222Z4+7n2v9PAvg2Wj9JNoqOEqKuN+5+qX3iNQF8ERmtiZkV0XK4L7v7t9rNma9J\naB4btSbtsW86aW6nbITzPwfggJndaWY9AD6CVjLQTDGzATMbuv4YwPsBHIv3Wlc2RULU6ydXmw8j\ngzUxMwPwJQDH3f1zN5gyXRM2j6zXJLOkuVntYC7bzfwAWjupJwD8hw2aw360lIYXAbyc5TwAfAWt\nr481tPY8PgZgO1plz14F8AMAIxs0j/8F4CUAR9sn23gG83gXWl9hjwJ4of3vA1mvSWQema4JgPsB\n/KI93jEA/7HdvqbroTv8hEiU1Df8hEgWOb8QiSLnFyJR5PxCJIqcX4hEkfMLkShyfiESRc4vRKL8\nXyIxizD0EAYKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3fdc2b1da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# pick a random number between 0 and 9999\n",
    "random = 12\n",
    "img = dataset[b'data'][random].reshape(3,32,32).transpose(1,2,0)\n",
    "plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6, 2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2, 2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2, 1, 1, 4, 9, 7, 8, 5, 9, 6, 7, 3, 1, 9, 0, 3, 1, 3, 5, 4, 5, 7, 7, 4, 7, 9, 4, 2, 3, 8, 0, 1, 6, 1, 1, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[b'labels'][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the first 100 labels - they are stored as a list of numbers between 0 and 9, where each number corresponds to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'horse'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "label_names[dataset[b'labels'][random]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, the selected label is a horse.\n",
    "\n",
    "We now need to break the dataset up into training and testing sets, so lets do that using the handy `train_test_split()` function from scikit-learn. First we turn the dataset into `x_data` for the actual images and `y_data` for the corresponding labels. Then we randomly split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data shape: (10000, 3072)\n",
      "y_data shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_data = np.array(dataset[b'data'])    # The training images\n",
    "y_data = np.array(dataset[b'labels'])  # The labels for the training images\n",
    "print(\"x_data shape:\", x_data.shape)\n",
    "print(\"y_data shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (9000, 3072)\n",
      "x_test shape: (1000, 3072)\n",
      "\n",
      "y_train shape: (9000,)\n",
      "y_test shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "n_samples = x_data.shape[0]\n",
    "indices = np.random.permutation(n_samples)\n",
    "training_idx, test_idx = indices[:int(0.9*n_samples)], indices[int(0.9*n_samples):]\n",
    "\n",
    "x_train, x_test = x_data[training_idx,:], x_data[test_idx,:]\n",
    "y_train, y_test = y_data[training_idx], y_data[test_idx]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print()\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have separate samples and labels both for training and for testing.\n",
    "\n",
    "## Making the Fully Connected Neural Network\n",
    "\n",
    "We know that the CIFAR-10 dataset is composed of 32x32 pictures that are in color (red, green, and blue color channels).  This means that images are in $\\mathbb{R}^{32*32*3} = \\mathbb{R}^{3072}$. Because there are 10 classes, the predictions are $\\hat{y} \\in \\mathbb{R}^{10}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_length = 3072\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to create the fully connected layers. To make this easier, we will first make a helper function to create a fully connected layer.\n",
    "\n",
    "Notice that we use `tf.variable_scope()` and `tf.get_variable()` instead of `tf.Variable()`. We use `tf.variable_scope()` to group together all the tensors and variables we define inside it. This does a couple of nice things:\n",
    "\n",
    "- Anything we define inside the variable scope will have its name prefixed with the name of the scope\n",
    "- If we try to display the graph, it will look much more organized (remember the tensorflow tutorial?)\n",
    "- We can control whether we reuse variables or not (more on that in a few sentences)\n",
    "\n",
    "Using `tf.get_variable()` will create a `tf.Variable` if the variable does not exist yet. If the variable already exists and the variable scope is in reuse mode, it will return the original variable without creating a new one. If not in reuse mode, it will throw an error, preventing us from accidentally duplicating variables\n",
    "For these reasons using `tf.variable_scope()` is a great organizational tool and helps us avoid making mistakes by preventing accidental variable duplication. If we had used `tf.Variable()`, it would ignore these duplication rules and always make a new variable, adding a suffix onto the name if it already existed.\n",
    "\n",
    "**In summary: Always keep your code organized and safe from sneaky bugs by using `tf.variable_scope()`, and always use `tf.get_variable()` instead of `tf.Variable()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc(input_tensor, output_features, name='FC', func=tf.nn.relu):\n",
    "    \"\"\"Creates a Fully Connected Layer\n",
    "\n",
    "    Args:\n",
    "        input_tensor:  Tensor of shape `[batch, features]` that this FC layer uses as its input features.\n",
    "        output_features:  The number of features that the layer will output.\n",
    "        name:  The name of the Fully Connected layer. Will use this to define the `tf.variable_scope()`.\n",
    "        func:  The activation function to use. If `None`, uses ReLU.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple of (activations, weights, bias). The weights and bias are returned so that a regularizer\n",
    "        can operate directly on them if needed. The activation Tensor represents the output feature \n",
    "        activations. Will have shape `[None, output_features]`.\n",
    "    \"\"\"\n",
    "    input_features = int(input_tensor.shape[1])  # Get the number of features for the input tensor\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('W', initializer=tf.truncated_normal(\n",
    "            shape=[input_features, output_features],\n",
    "            stddev=np.sqrt(2/(input_features*output_features))))  # Set stddev of random distribution for weights\n",
    "        b = tf.get_variable('B', initializer=tf.zeros([output_features]))\n",
    "        \n",
    "        return func(tf.matmul(input_tensor, w) + b, name='Activations'), w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we declare all our inputs, outputs, and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()  # Clear the graph to avoid errors from reusing variables\n",
    "\n",
    "with tf.variable_scope('Inputs'):\n",
    "    x = tf.placeholder(tf.float32, [None, input_length], name='x')\n",
    "    y = tf.placeholder(tf.int64, [None,], name='y')  # Last time we one-hot encoded our labels. Now we won't.\n",
    "\n",
    "with tf.variable_scope('Hidden-Layers'):\n",
    "    hidden_features = 1024\n",
    "    hidden1, w1, _ = fc(x, hidden_features, 'FC1')\n",
    "    hidden2, w2, _ = fc(hidden1, hidden_features, 'FC2')\n",
    "    hidden3, w3, _ = fc(hidden2, hidden_features, 'FC3')\n",
    "    \n",
    "with tf.variable_scope('Softmax'):\n",
    "    w = tf.get_variable('W', initializer=tf.truncated_normal(\n",
    "        shape=[hidden_features, num_classes],\n",
    "        stddev=np.sqrt(1/(hidden_features*num_classes)))) # Set stddev of random distribution for weights\n",
    "    b = tf.get_variable('B', initializer=tf.zeros([num_classes]))\n",
    "    \n",
    "    scores = tf.matmul(hidden3, w) + b\n",
    "    # Predicted probability vectors for each sample in the batch, shape = `[None, 10]`\n",
    "    pred = tf.nn.softmax(scores)\n",
    "\n",
    "with tf.variable_scope('Optimization'):\n",
    "    # Last time we used the regular cross entropy function, but this time we use the \"sparse\" version. \n",
    "    # That's because this version takes care of turning the labels into one hot encodings for us!\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores))\n",
    "    \n",
    "    regularizer = (tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2) + tf.nn.l2_loss(w3) + tf.nn.l2_loss(w))  # L2 Regularizer\n",
    "    lmbda = 0.1412  # Regularizer coefficient\n",
    "    loss += lmbda*regularizer  # Add regularization penalty to the loss function\n",
    "    \n",
    "    correct = tf.equal(tf.argmax(pred, axis=1), y)           # boolean 1-D Tensor of if pred was correct\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))  # scalar (0-D) Tensor of the average accuracy\n",
    "    \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate=0.000005).minimize(loss)  # Op that steps loss towards minimum\n",
    "    \n",
    "init = tf.global_variables_initializer()  # Op that initializes variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "We already split that dataset into training and testing sets. Now we just need to feed the training set into the model and perform gradient descent. Because the training set is large (9000 images), we will feed in only a subset of the training set at a time for each batch.\n",
    "\n",
    "*Side note: When using the whole dataset in a batch, it is called \"Gradient Descent\". When using a subset of the dataset in the batch, this is called \"Mini-Batch Gradient Descent\". When the batch size is set to 1, it is called \"Stochastic Gradient Descent\". People mix up these terms, so if you hear someone saying \"use SGD\", they probably are referring to Mini-Batch Gradient Descent rather than Stochastic Gradient Descent. Even in academic papers, people confuse these!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)  # Initialize all `tf.Variable` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      1, loss=2.596647, regularizer=2.1253, accuracy=10.56%\n",
      "epoch     10, loss=2.117517, regularizer=1.1166, accuracy=26.32%\n",
      "epoch     20, loss=2.003032, regularizer=1.0295, accuracy=31.42%\n",
      "epoch     30, loss=1.948051, regularizer=0.9987, accuracy=33.70%\n",
      "epoch     40, loss=1.913923, regularizer=0.9929, accuracy=34.86%\n",
      "epoch     50, loss=1.882845, regularizer=0.9977, accuracy=36.62%\n"
     ]
    }
   ],
   "source": [
    "model_name = 'fcn-trained.ckpt'\n",
    "\n",
    "n_epochs = 1000     # The number of full passes through the dataset before we quit training\n",
    "batch_size= 256  # Feed in only 256 images in a single batch instead of all 9,000\n",
    "\n",
    "training_size = x_train.shape[0]\n",
    "\n",
    "saver = tf.train.Saver() # Allows us to save and restore a model\n",
    "#saver.restore(sess, model_name)\n",
    "\n",
    "\n",
    "for j in range(n_epochs):\n",
    "    perm = np.random.permutation(training_size)  # Every epoch, get a new set of batches\n",
    "    for i in range(0, training_size, batch_size):\n",
    "        idx = perm[i:i+batch_size]  # Select indices for batch\n",
    "        x_batch = x_train[idx]\n",
    "        y_batch = y_train[idx]\n",
    "        sess.run(train_step, feed_dict={x:x_batch, y:y_batch})\n",
    "    if j%10 == 9 or j==0:\n",
    "        l, r, a = sess.run([loss, regularizer, accuracy], feed_dict={x:x_train, y:y_train})\n",
    "        print(\"epoch %6d, loss=%6f, regularizer=%0.4f, accuracy=%.2f%%\" % (j+1, l, round(r, 4), 100*round(a, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.save(sess, model_name)  # Save the trained model in the file specified by `model_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate some predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random image from our test set (1000 images)\n",
    "random = 65\n",
    "sample = x_test[random]\n",
    "plt.imshow(sample.reshape(3,32,32).transpose(1,2,0));\n",
    "plt.title('Selected Image')\n",
    "\n",
    "print('Prediction:', label_names[sess.run(tf.argmax(pred, axis=1), feed_dict={x:[sample]})[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy:', sess.run(accuracy, feed_dict={x:x_train, y:y_train}))\n",
    "print('Testing Accuracy:', sess.run(accuracy, feed_dict={x:x_test, y:y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Regularization\n",
    "When we train our model (i.e. optimize the weights), we are optimizing the performance of our model on the training data. However, we have to be careful because our model may become too specific to the training data we provided and not be able to generalize to new data in the test set that it has not seen - we refer to this as overfitting. To avoid this, we add a term to our loss function that measures the \"size\" (L2 norm) of each $W$, so that we do not get a weight matrix with extremely large values. \n",
    "\n",
    "To see why smaller weights are desirable, consider the image below:\n",
    "<img src ='https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/Regularization.svg/354px-Regularization.svg.png' style=\"width: 250px;\"/>\n",
    "When we use highly complex models such as neural networks, it is desirable for the weights to be small so that the function does not fluctuate wildly with small changes to our inputs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
